{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CACHE_DIR = \"/share/edc/home/antonis/datasets/huggingface\"\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = CACHE_DIR\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token counts from /share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_0/token_counts.json\n",
      "3.41e+10\n",
      "6.70e+06\n",
      "Number of tokens in training set: 3.41e+10\n",
      "Number of parameters in GPT2: 1.24e+08\n",
      "Ratio: 275.2673945887097\n",
      "No. Parameters required: 1.71e+09\n",
      "Loading token counts from /share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_1/token_counts.json\n",
      "3.39e+10\n",
      "6.70e+06\n",
      "Number of tokens in training set: 3.39e+10\n",
      "Number of parameters in GPT2: 1.24e+08\n",
      "Ratio: 273.6619662096774\n",
      "No. Parameters required: 1.70e+09\n",
      "Token difference: 1.99e+08\n",
      "Token difference ratio: 0.5832250424831484%\n"
     ]
    }
   ],
   "source": [
    "def print_stats(token_counts_dir):\n",
    "    # Load the token counts\n",
    "    print(f\"Loading token counts from {token_counts_dir}\")\n",
    "    with open(token_counts_dir, \"r\") as f:\n",
    "        token_counts = json.load(f)\n",
    "\n",
    "    # Print the token counts with scientific notation\n",
    "    for val in token_counts.values():\n",
    "        print(f\"{val:.2e}\")\n",
    "\n",
    "    # Calculate and print the statistics\n",
    "    n_tokens_training = token_counts[\"train\"]\n",
    "    n_params_gpt2 = 124e6\n",
    "\n",
    "    print(f\"Number of tokens in training set: {n_tokens_training:.2e}\")\n",
    "    print(f\"Number of parameters in GPT2: {n_params_gpt2:.2e}\")\n",
    "    print(f\"Ratio: {n_tokens_training/n_params_gpt2}\")\n",
    "\n",
    "    # Calculate required parameters and tokens\n",
    "    required_ratio = 20\n",
    "    n_tokens_required = required_ratio * n_params_gpt2\n",
    "    n_parameters_required = n_tokens_training / required_ratio\n",
    "\n",
    "    print(f\"No. Parameters required: {n_parameters_required:.2e}\")\n",
    "    return token_counts\n",
    "\n",
    "# Usage:\n",
    "ds_dir = \"/share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_0\"\n",
    "token_counts_dir = os.path.join(ds_dir, \"token_counts.json\")\n",
    "token_counts = print_stats(token_counts_dir)\n",
    "\n",
    "ds_dir_2 = \"/share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_1\"\n",
    "token_counts_dir_2 = os.path.join(ds_dir_2, \"token_counts.json\")\n",
    "token_counts_2 = print_stats(token_counts_dir_2)\n",
    "\n",
    "token_diff = token_counts[\"train\"] - token_counts_2[\"train\"]\n",
    "print(f\"Token difference: {token_diff:.2e}\")\n",
    "\n",
    "token_diff_ratio = token_diff / token_counts[\"train\"]\n",
    "print(f\"Token difference ratio: {token_diff_ratio * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def count_gpt2_tokens(dataset, text_column):\n",
    "    \"\"\"\n",
    "    Tokenize a Hugging Face Dataset using GPT-2 tokenizer and count the total number of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (datasets.Dataset): Hugging Face Dataset to tokenize.\n",
    "    text_column (str): Name of the column in the dataset that contains the text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    int: Total number of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pre-trained GPT-2 tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    # Define a function to tokenize each example and return the number of tokens\n",
    "    def count_tokens(example):\n",
    "        tokens = tokenizer.encode(example[text_column], truncation=True)\n",
    "        return {\"num_tokens\": len(tokens)}\n",
    "\n",
    "    # Map the count_tokens function to the dataset\n",
    "    dataset = dataset.map(count_tokens, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Sum the num_tokens column to get the total number of tokens\n",
    "    num_tokens = sum(dataset['num_tokens'])\n",
    "\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.41k/4.41k [00:00<00:00, 4.21MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.04k/2.04k [00:00<00:00, 2.37MB/s]\n",
      "Downloading readme: 100%|██████████| 6.55k/6.55k [00:00<00:00, 7.96MB/s]\n",
      "Found cached dataset yelp_review_full (/share/edc/home/antonis/datasets/huggingface/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "Downloading builder script: 100%|██████████| 4.03k/4.03k [00:00<00:00, 2.36MB/s]\n",
      "Downloading metadata: 100%|██████████| 1.59k/1.59k [00:00<00:00, 1.28MB/s]\n",
      "Downloading readme: 100%|██████████| 6.84k/6.84k [00:00<00:00, 7.19MB/s]\n",
      "Found cached dataset sentiment140 (/share/edc/home/antonis/datasets/huggingface/sentiment140/sentiment140/1.0.0/f81c014152931b776735658d8ae493b181927de002e706c4d5244ecb26376997)\n"
     ]
    }
   ],
   "source": [
    "ds_yelp_review = load_dataset(\"yelp_review_full\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "ds_sentiment140 = load_dataset(\"sentiment140\", split=\"train\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113910634"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_gpt2_tokens(ds_yelp_review, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of full dataset to training dataset: 0.43369263882600073%\n"
     ]
    }
   ],
   "source": [
    "ds_yelp_review_tokens = 113910634\n",
    "ds_sentiment140_tokens = 34122355\n",
    "\n",
    "\"\"\"\n",
    "ds_yelp_review_tokens = 113910634\n",
    "ds_sentiment140_tokens = 34122355\n",
    "\"\"\"\n",
    "\n",
    "full_xy = ds_yelp_review_tokens + ds_sentiment140_tokens\n",
    "\n",
    "with open(token_counts_dir, \"r\") as f:\n",
    "    token_counts = json.load(f)\n",
    "\n",
    "xy_ratio = full_xy / token_counts[\"train\"]\n",
    "print(f\"Ratio of full dataset to training dataset: {xy_ratio * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_xy: 1.48e+08\n"
     ]
    }
   ],
   "source": [
    "print(f\"full_xy: {full_xy:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "ds_sentiment140_tokens = count_gpt2_tokens(ds_sentiment140, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34122355\n"
     ]
    }
   ],
   "source": [
    "print(ds_sentiment140_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_xy = len(ds_yelp_review) + len(ds_sentiment140)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
