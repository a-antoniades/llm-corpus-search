{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token counts from /share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_0/token_counts.json\n",
      "3.41e+10\n",
      "6.70e+06\n",
      "Number of tokens in training set: 3.41e+10\n",
      "Number of parameters in GPT2: 1.24e+08\n",
      "Ratio: 275.2673945887097\n",
      "No. Parameters required: 1.71e+09\n",
      "Loading token counts from /share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_1/token_counts.json\n",
      "3.39e+10\n",
      "6.70e+06\n",
      "Number of tokens in training set: 3.39e+10\n",
      "Number of parameters in GPT2: 1.24e+08\n",
      "Ratio: 273.6619662096774\n",
      "No. Parameters required: 1.70e+09\n",
      "Token difference: 1.99e+08\n",
      "Token difference ratio: 0.5832250424831484%\n"
     ]
    }
   ],
   "source": [
    "def print_stats(token_counts_dir):\n",
    "    # Load the token counts\n",
    "    print(f\"Loading token counts from {token_counts_dir}\")\n",
    "    with open(token_counts_dir, \"r\") as f:\n",
    "        token_counts = json.load(f)\n",
    "\n",
    "    # Print the token counts with scientific notation\n",
    "    for val in token_counts.values():\n",
    "        print(f\"{val:.2e}\")\n",
    "\n",
    "    # Calculate and print the statistics\n",
    "    n_tokens_training = token_counts[\"train\"]\n",
    "    n_params_gpt2 = 124e6\n",
    "\n",
    "    print(f\"Number of tokens in training set: {n_tokens_training:.2e}\")\n",
    "    print(f\"Number of parameters in GPT2: {n_params_gpt2:.2e}\")\n",
    "    print(f\"Ratio: {n_tokens_training/n_params_gpt2}\")\n",
    "\n",
    "    # Calculate required parameters and tokens\n",
    "    required_ratio = 20\n",
    "    n_tokens_required = required_ratio * n_params_gpt2\n",
    "    n_parameters_required = n_tokens_training / required_ratio\n",
    "\n",
    "    print(f\"No. Parameters required: {n_parameters_required:.2e}\")\n",
    "\n",
    "    return token_counts\n",
    "\n",
    "# Usage:\n",
    "ds_dir = \"/share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_0\"\n",
    "token_counts_dir = os.path.join(ds_dir, \"token_counts.json\")\n",
    "token_counts = print_stats(token_counts_dir)\n",
    "\n",
    "ds_dir_2 = \"/share/edc/home/antonis/datasets/huggingface/merged_datasets/sentiment_c4/P_1_PQA_5_promptsource_True/dataset_1\"\n",
    "token_counts_dir_2 = os.path.join(ds_dir_2, \"token_counts.json\")\n",
    "token_counts_2 = print_stats(token_counts_dir_2)\n",
    "\n",
    "token_diff = token_counts[\"train\"] - token_counts_2[\"train\"]\n",
    "print(f\"Token difference: {token_diff:.2e}\")\n",
    "\n",
    "token_diff_ratio = token_diff / token_counts[\"train\"]\n",
    "print(f\"Token difference ratio: {token_diff_ratio * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
