{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/antonis/.conda/envs/incidental/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from promptsource import templates\n",
    "\n",
    "CACHE_DIR = \"/share/edc/home/antonis/datasets/huggingface\"\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = CACHE_DIR\n",
    "\n",
    "# # Get a list of all supported datasets\n",
    "# datasets = templates.get_dataset_names()\n",
    "# print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from promptsource.templates import DatasetTemplates, TemplateCollection\n",
    "# set logging level to INFO\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(20)\n",
    "\n",
    "TOMixture = [\n",
    "    # (\"glue\",\"mrpc\"), # Paraphrase identification\n",
    "    # (\"glue\",\"qqp\"),\n",
    "    # (\"paws\",\"labeled_final\"),\n",
    "    # (\"kilt_tasks\", \"hotpotqa\"), # Closed-book QA\n",
    "    # (\"wiki_qa\", None),\n",
    "    # (\"adversarial_qa\", \"dbidaf\"), # Extractive QA\n",
    "    # (\"adversarial_qa\",\"dbert\"),\n",
    "    # (\"adversarial_qa\",\"droberta\"),\n",
    "    # (\"duorc\",\"SelfRC\"),\n",
    "    # (\"duorc\",\"ParaphraseRC\"),\n",
    "    # (\"ropes\",None),\n",
    "    # (\"quoref\",None),\n",
    "    # (\"cos_e\",\"v1.11\"), # Multiple-choice QA\n",
    "    # (\"cosmos_qa\",None),\n",
    "    # (\"dream\",None),\n",
    "    # (\"qasc\",None),\n",
    "    # (\"quail\",None),\n",
    "    # (\"quarel\",None),\n",
    "    # (\"quartz\",None),\n",
    "    # (\"sciq\",None),\n",
    "    # (\"social_i_qa\",None),\n",
    "    # (\"wiki_hop\",\"original\"),\n",
    "    # (\"wiqa\",None),\n",
    "    # (\"amazon_polarity\",None), # Sentiment\n",
    "    # (\"app_reviews\",None),\n",
    "    (\"sst\",\"default\"), # Senitment Classification\")\n",
    "    (\"imdb\",None),\n",
    "    # (\"rotten_tomatoes\",None),\n",
    "    # (\"yelp_review_full\",None),\n",
    "    # (\"common_gen\",None), # Structure-to-text\n",
    "    # (\"wiki_bio\",None),\n",
    "    # (\"cnn_dailymail\",\"3.0.0\"), # Summarization\n",
    "    # (\"gigaword\",None),\n",
    "    # (\"multi_news\",None),\n",
    "    # (\"samsum\",None),\n",
    "    # (\"xsum\",None),\n",
    "    # (\"ag_news\",None), # Topic Classification\n",
    "    # (\"dbpedia_14\",None),\n",
    "    # (\"trec\",None),\n",
    "]\n",
    "\n",
    "def get_dataset_name(name: str, subset: str):\n",
    "    if subset is not None:\n",
    "        canonized_name = f\"{name}/{subset}\"\n",
    "    else:\n",
    "        canonized_name = name\n",
    "    return canonized_name\n",
    "\n",
    "def get_T0MixtureDatasets(split, max_samples=None, return_as_dict=True):\n",
    "    \"\"\"\n",
    "    T0MixtureDatasets creates a separate dataset for each dataset in the mixture\n",
    "    \"\"\"\n",
    "    datasets = {} if return_as_dict else []\n",
    "    for name, subset in TOMixture:\n",
    "        dataset = load_dataset(name, subset, split=split, cache_dir=CACHE_DIR)\n",
    "        if max_samples:\n",
    "            dataset = Dataset.from_dict(dataset[:max_samples])\n",
    "        templates = [template for id, template in DatasetTemplates(name, subset).templates.items()]\n",
    "        dataset.templates = templates\n",
    "        dataset.name = get_dataset_name(name, subset)\n",
    "\n",
    "        if return_as_dict:\n",
    "            datasets[get_dataset_name(name, subset)] = dataset\n",
    "        else:\n",
    "            datasets.append(dataset)\n",
    "\n",
    "\n",
    "        logger.info(f\"Loaded dataset {name}/{subset} with {len(templates)} templates\")\n",
    "        assert(len(templates) > 0), \"No templates\"\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from promptsource.templates import DatasetTemplates, TemplateCollection\n",
    "# set logging level to INFO\n",
    "from src._promptsource import get_T0MixtureDatasets, TOMixture \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datasets \u001b[39m=\u001b[39m get_T0MixtureDatasets(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_samples\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, return_as_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m ds_sst \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39msst\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, cache_dir\u001b[39m=\u001b[39mCACHE_DIR)\n\u001b[1;32m      3\u001b[0m ds_imdb \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mimdb\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, cache_dir\u001b[39m=\u001b[39mCACHE_DIR)\n",
      "File \u001b[0;32m/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/src/_promptsource.py:79\u001b[0m, in \u001b[0;36mget_T0MixtureDatasets\u001b[0;34m(split, max_samples, return_as_dict)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mT0MixtureDatasets creates a separate dataset for each dataset in the mixture\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m datasets \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m return_as_dict \u001b[39melse\u001b[39;00m []\n\u001b[0;32m---> 79\u001b[0m \u001b[39mfor\u001b[39;00m name, subset \u001b[39min\u001b[39;00m TOMixture:\n\u001b[1;32m     80\u001b[0m     dataset \u001b[39m=\u001b[39m load_dataset(name, subset, split\u001b[39m=\u001b[39msplit, cache_dir\u001b[39m=\u001b[39mCACHE_DIR)\n\u001b[1;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m max_samples:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not iterable"
     ]
    }
   ],
   "source": [
    "datasets = get_T0MixtureDatasets(\"test\", max_samples=1000, return_as_dict=True)\n",
    "ds_sst = load_dataset(\"sst\", \"default\", split=\"test\", cache_dir=CACHE_DIR)\n",
    "ds_imdb = load_dataset(\"imdb\", split=\"test\", cache_dir=CACHE_DIR)\n",
    "# ds_yelp_review = load_dataset(\"yelp_review_full\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "# ds_sentiment140 = load_dataset(\"sentiment140\", split=\"train\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
