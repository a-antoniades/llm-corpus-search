{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from promptsource import templates\n",
    "\n",
    "CACHE_DIR = \"/share/edc/home/antonis/datasets/huggingface\"\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = CACHE_DIR\n",
    "\n",
    "# # Get a list of all supported datasets\n",
    "# datasets = templates.get_dataset_names()\n",
    "# print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/antonis/.conda/envs/incidental/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from promptsource.templates import DatasetTemplates, TemplateCollection\n",
    "# set logging level to INFO\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(20)\n",
    "\n",
    "TOMixture = [\n",
    "    # (\"glue\",\"mrpc\"), # Paraphrase identification\n",
    "    # (\"glue\",\"qqp\"),\n",
    "    # (\"paws\",\"labeled_final\"),\n",
    "    # (\"kilt_tasks\", \"hotpotqa\"), # Closed-book QA\n",
    "    # (\"wiki_qa\", None),\n",
    "    # (\"adversarial_qa\", \"dbidaf\"), # Extractive QA\n",
    "    # (\"adversarial_qa\",\"dbert\"),\n",
    "    # (\"adversarial_qa\",\"droberta\"),\n",
    "    # (\"duorc\",\"SelfRC\"),\n",
    "    # (\"duorc\",\"ParaphraseRC\"),\n",
    "    # (\"ropes\",None),\n",
    "    # (\"quoref\",None),\n",
    "    # (\"cos_e\",\"v1.11\"), # Multiple-choice QA\n",
    "    # (\"cosmos_qa\",None),\n",
    "    # (\"dream\",None),\n",
    "    # (\"qasc\",None),\n",
    "    # (\"quail\",None),\n",
    "    # (\"quarel\",None),\n",
    "    # (\"quartz\",None),\n",
    "    # (\"sciq\",None),\n",
    "    # (\"social_i_qa\",None),\n",
    "    # (\"wiki_hop\",\"original\"),\n",
    "    # (\"wiqa\",None),\n",
    "    # (\"amazon_polarity\",None), # Sentiment\n",
    "    # (\"app_reviews\",None),\n",
    "    (\"sst\",\"default\"), # Senitment Classification\")\n",
    "    (\"imdb\",None),\n",
    "    # (\"rotten_tomatoes\",None),\n",
    "    # (\"yelp_review_full\",None),\n",
    "    # (\"common_gen\",None), # Structure-to-text\n",
    "    # (\"wiki_bio\",None),\n",
    "    # (\"cnn_dailymail\",\"3.0.0\"), # Summarization\n",
    "    # (\"gigaword\",None),\n",
    "    # (\"multi_news\",None),\n",
    "    # (\"samsum\",None),\n",
    "    # (\"xsum\",None),\n",
    "    # (\"ag_news\",None), # Topic Classification\n",
    "    # (\"dbpedia_14\",None),\n",
    "    # (\"trec\",None),\n",
    "]\n",
    "\n",
    "def get_dataset_name(name: str, subset: str):\n",
    "    if subset is not None:\n",
    "        canonized_name = f\"{name}/{subset}\"\n",
    "    else:\n",
    "        canonized_name = name\n",
    "    return canonized_name\n",
    "\n",
    "def get_T0MixtureDatasets(split, max_samples=None, return_as_dict=True):\n",
    "    \"\"\"\n",
    "    T0MixtureDatasets creates a separate dataset for each dataset in the mixture\n",
    "    \"\"\"\n",
    "    datasets = {} if return_as_dict else []\n",
    "    for name, subset in TOMixture:\n",
    "        dataset = load_dataset(name, subset, split=split, cache_dir=CACHE_DIR)\n",
    "        if max_samples:\n",
    "            dataset = Dataset.from_dict(dataset[:max_samples])\n",
    "        templates = [template for id, template in DatasetTemplates(name, subset).templates.items()]\n",
    "        dataset.templates = templates\n",
    "        dataset.name = get_dataset_name(name, subset)\n",
    "\n",
    "        if return_as_dict:\n",
    "            datasets[get_dataset_name(name, subset)] = dataset\n",
    "        else:\n",
    "            datasets.append(dataset)\n",
    "\n",
    "\n",
    "        logger.info(f\"Loaded dataset {name}/{subset} with {len(templates)} templates\")\n",
    "        assert(len(templates) > 0), \"No templates\"\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "from promptsource.templates import DatasetTemplates, TemplateCollection\n",
    "# set logging level to INFO\n",
    "# from src._promptsource import get_T0MixtureDatasets \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/share/edc/home/antonis/datasets/huggingface/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "Found cached dataset imdb (/share/edc/home/antonis/datasets/huggingface/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset sst (/share/edc/home/antonis/datasets/huggingface/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "Found cached dataset imdb (/share/edc/home/antonis/datasets/huggingface/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "datasets = get_T0MixtureDatasets(\"test\", max_samples=1000, return_as_dict=True)\n",
    "ds_sst = load_dataset(\"sst\", \"default\", split=\"test\", cache_dir=CACHE_DIR)\n",
    "ds_imdb = load_dataset(\"imdb\", split=\"test\", cache_dir=CACHE_DIR)\n",
    "# ds_yelp_review = load_dataset(\"yelp_review_full\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "# ds_sentiment140 = load_dataset(\"sentiment140\", split=\"train\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_nli (/share/edc/home/antonis/datasets/huggingface/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    }
   ],
   "source": [
    "name, subset = \"multi_nli\", None\n",
    "dataset = load_dataset(name, subset, split=\"train\", cache_dir=CACHE_DIR)\n",
    "templates = [template for id, template in DatasetTemplates(name, subset).templates.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['answer_choices', 'id', 'jinja', 'metadata', 'name', 'reference'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['True', 'Inconclusive', 'False']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(templates[0].__dict__.keys())\n",
    "templates[0].answer_choices.split(' ||| ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "template = random.choice(templates)\n",
    "prompt, answer = template.apply(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Conceptually cream skimming has two basic dimensions - product and geography. Are we justified in saying that \"Product and geography are what make cream skimming work. \"? Yes, no, or maybe?',\n",
       " 'Maybe')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt, answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
