{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /local/home/antonis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /local/home/antonis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[dynet] random seed: 1234\n",
      "[dynet] allocating memory: 32MB\n",
      "[dynet] memory allocation done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from src.wimbd_ import _load_dataset\n",
    "\n",
    "HF_HOME = \"/share/edc/home/antonis/datasets/huggingface\"\n",
    "save_path = os.makedirs(os.path.join(HF_HOME, 'wmt09_gens'), exist_ok=True)\n",
    "\n",
    "base_pth = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/experiment_6_logits_max_4/inference/allenai/OLMo-7B/TRANSLATION\"\n",
    "doc_res_files = glob.glob(os.path.join(base_pth, \"**/doc_results.json\"), recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'wmt\\d{2}-(\\w{2}-\\w{2})')\n",
    "language_pairs = [pattern.search(path).group(1) for path in doc_res_files if pattern.search(path)]\n",
    "doc = json.load(open(doc_res_files[0]))\n",
    "\n",
    "def find_lang(path):\n",
    "    pattern = re.compile(r'wmt\\d{2}-(\\w{2}-\\w{2})')\n",
    "    return pattern.search(path).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': 'In New York, Wall Street opened with significant losses.',\n",
       " 'ref': 'New Yorkban a Wall Street jelentős veszteséggel nyitott.',\n",
       " 'result': [' A New York-i Wall Street előtt körülbelül 30 perc alatt kezdődik.'],\n",
       " 'bleu': 8.054496384843702,\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['wmt09-en-hu'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wmt09-en-hu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_res_files[0].split('/')[-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset wmt\n",
      "Loading dataset for language pair ('cs', 'en')\n",
      "Loading dataset for language pair ('de', 'en')\n",
      "Loading dataset for language pair ('fr', 'en')\n",
      "Loading dataset for language pair ('es', 'en')\n",
      "Loading dataset for language pair ('it', 'en')\n",
      "Loading dataset for language pair ('hu', 'en')\n",
      "Loading dataset for language pair ('en', 'cs')\n",
      "Loading dataset for language pair ('en', 'de')\n",
      "Loading dataset for language pair ('en', 'fr')\n",
      "Loading dataset for language pair ('en', 'es')\n",
      "Loading dataset for language pair ('en', 'it')\n",
      "Loading dataset for language pair ('en', 'hu')\n"
     ]
    }
   ],
   "source": [
    "wmt = _load_dataset('wmt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Prague Stock Market falls to minus by the end of the trading day', 'gen': ' A New York-i Wall Street előtt körülbelül 30 perc alatt kezdődik.', 'hu': 'A prágai tőzsde a kereskedés végére mínuszba került'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lang1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatched \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatched_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wmt\n\u001b[0;32m---> 50\u001b[0m wmt \u001b[38;5;241m=\u001b[39m \u001b[43minsert_gens_into_wmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_res_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m {lang: Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m: value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m]}) \u001b[38;5;28;01mfor\u001b[39;00m lang, value \u001b[38;5;129;01min\u001b[39;00m wmt\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     53\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m DatasetDict(dataset_dict)\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36minsert_gens_into_wmt\u001b[0;34m(wmt, doc_res_files)\u001b[0m\n\u001b[1;32m     27\u001b[0m row \u001b[38;5;241m=\u001b[39m doc_res_task_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(row)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m src \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m ref:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lang1'"
     ]
    }
   ],
   "source": [
    "wmt['cs-en']['translation']\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "\n",
    "def insert_gens_into_wmt(wmt, doc_res_files):\n",
    "    for path in tqdm(doc_res_files, desc=\"Processing files\"):\n",
    "        with open(path, 'r') as f:\n",
    "            doc_res = json.load(f)\n",
    "        \n",
    "        task = path.split('/')[-3]\n",
    "        lang = find_lang(path)\n",
    "        lang1, lang2 = lang.split('-')\n",
    "        total_rows = len(wmt[lang]['translation'])\n",
    "        matched_rows = 0\n",
    "        doc_res_task_dict = wmt[lang].to_dict()\n",
    "        len_task_dict = len(doc_res_task_dict['translation'])\n",
    "        unmatched_rows = [i for i in range(len_task_dict)]\n",
    "\n",
    "        for doc in tqdm(doc_res[task], desc=f\"Inserting generations for {task}\", leave=False):\n",
    "            src = doc['src']\n",
    "            ref = doc['ref']\n",
    "            gen = doc['result'][0]\n",
    "\n",
    "            for idx in unmatched_rows:\n",
    "                row = doc_res_task_dict['translation'][idx]\n",
    "                print(row)\n",
    "                if row[lang1] != src and row[lang2] != ref:\n",
    "                    continue\n",
    "                else:\n",
    "                    doc_res_task_dict['translation'][idx]['gen'] = gen\n",
    "                    matched_rows += 1\n",
    "                    unmatched_rows.remove(idx)\n",
    "                    break\n",
    "        \n",
    "        if len(unmatched_rows) > 0:\n",
    "            print(f\"Could not match {len(unmatched_rows)} rows for {task}\")\n",
    "\n",
    "        wmt[lang] = Dataset.from_dict(doc_res_task_dict)\n",
    "        dataset_dict = {lang: Dataset.from_dict({'translation': value['translation']}) for lang, value in wmt.items()}\n",
    "        dataset_dict = DatasetDict(dataset_dict)\n",
    "        # Now you can save it to disk\n",
    "        dataset_dict.save_to_disk(os.path.join(HF_HOME, 'wmt09_gens'))\n",
    "        print(f\"Matched {matched_rows}/{total_rows} rows for {task}\")\n",
    "\n",
    "    return wmt\n",
    "\n",
    "\n",
    "wmt = insert_gens_into_wmt(wmt, doc_res_files)\n",
    "\n",
    "dataset_dict = {lang: Dataset.from_dict({'translation': value['translation']}) for lang, value in wmt.items()}\n",
    "dataset_dict = DatasetDict(dataset_dict)\n",
    "\n",
    "# Now you can save it to disk\n",
    "dataset_dict.save_to_disk(os.path.join(HF_HOME, 'wmt09_gens'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
