{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.options.display.max_rows = 500\n",
    "# pd.options.display.max_columns = 20\n",
    "# pd.set_option('display.max_columns', 200)\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('display.min_rows', 100)\n",
    "# pd.set_option('display.expand_frame_repr', True)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from src.wimbd_ import _load_dataset, WimbdTasks\n",
    "wt = WimbdTasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits_pth = \"./models/experiment_6_logits/inference/EleutherAI/pythia-14m/TRANSLATION/wmt09-cs-en/0-shot/logits.pt\"\n",
    "# logits_pth = \"./models/experiment_6_logits_max/inference/EleutherAI/pythia-14m/TRANSLATION/wmt09-cs-en/0-shot/logits.pt\"\n",
    "logits_pth = \"./models/experiment_6_logits_max_2/inference/EleutherAI/pythia-14m/TRANSLATION/wmt09-fr-en/0-shot/logits/logits_part_0.pt\"\n",
    "logits = torch.load(logits_pth)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from collections import defaultdict\n",
    "# import pandas as pd\n",
    "\n",
    "# directory = \"./models/experiment_6_logits_max_2\"\n",
    "# shape_distribution_per_model = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# # Walk through the directory\n",
    "# for dirpath, _, filenames in os.walk(directory):\n",
    "#     for filename in filenames:\n",
    "#         if filename.endswith('.pt'):\n",
    "#             # Get the parent directory of the file to use as the model name\n",
    "#             model_name = os.path.basename(os.path.dirname(dirpath))\n",
    "#             file_path = os.path.join(dirpath, filename)\n",
    "#             tensor = torch.load(file_path).flatten()\n",
    "#             tensor_shape = tensor.shape[0]  # Record the flattened size\n",
    "#             shape_distribution_per_model[model_name][tensor_shape] += 1\n",
    "\n",
    "# # Convert the distribution data into a list of tuples (model, tensor_size, count)\n",
    "# data_for_plotting = []\n",
    "# for model, shape_counts in shape_distribution_per_model.items():\n",
    "#     for tensor_size, count in shape_counts.items():\n",
    "#         data_for_plotting.append((model, tensor_size, count))\n",
    "\n",
    "# # Convert the list into a DataFrame\n",
    "# df = pd.DataFrame(data_for_plotting, columns=['Model', 'Tensor Size', 'Count'])\n",
    "\n",
    "# # Now you can plot df using matplotlib or seaborn as shown previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3027/3027 [00:04<00:00, 654.08it/s] \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def load_logits(logits_pth):\n",
    "    logit_dict = {}\n",
    "    all_pths = sorted(glob.glob(os.path.join(logits_pth, \"**/*.pt\"), recursive=True))\n",
    "    for pth in tqdm(all_pths):\n",
    "        logits = torch.load(pth)\n",
    "        id_ = int(pth.split(\"_\")[-1].split(\".\")[0])\n",
    "        logit_dict[id_] = logits\n",
    "        # print(f\"id: {id_}, logits shape: {logits.shape}\")\n",
    "    return logit_dict\n",
    "\n",
    "model_logits_pth = \"./models/experiment_6_logits_max_3/inference/EleutherAI/pythia-12b/TRANSLATION/wmt09-en-fr\"\n",
    "model_logits = load_logits(model_logits_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_1</th>\n",
       "      <th>lang_2</th>\n",
       "      <th>value</th>\n",
       "      <th>example</th>\n",
       "      <th>coverage</th>\n",
       "      <th>task</th>\n",
       "      <th>query</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "      <th>id</th>\n",
       "      <th>gold</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>result</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>temps après</td>\n",
       "      <td>first time</td>\n",
       "      <td>186.0</td>\n",
       "      <td>[temps après, first time]</td>\n",
       "      <td>0.066915</td>\n",
       "      <td>fr-en</td>\n",
       "      <td>['temps après', 'first time']</td>\n",
       "      <td>186.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2947_4</td>\n",
       "      <td>0</td>\n",
       "      <td>Peu de temps après, le témoin, qui au début n'...</td>\n",
       "      <td>Shortly after, the witness, who, in a first ti...</td>\n",
       "      <td>A few minutes later, the witness, who at firs...</td>\n",
       "      <td>28.795572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lang_1      lang_2  value                    example  coverage   task  \\\n",
       "0  temps après  first time  186.0  [temps après, first time]  0.066915  fr-en   \n",
       "\n",
       "                           query    sum  count      id  gold  \\\n",
       "0  ['temps après', 'first time']  186.0      1  2947_4     0   \n",
       "\n",
       "                                                 src  \\\n",
       "0  Peu de temps après, le témoin, qui au début n'...   \n",
       "\n",
       "                                                 ref  \\\n",
       "0  Shortly after, the witness, who, in a first ti...   \n",
       "\n",
       "                                              result       bleu  \n",
       "0   A few minutes later, the witness, who at firs...  28.795572  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_common_pth = \"./results/n-grams/exp_full/2/examples_dfs_0-shot_common_models.pkl\"\n",
    "# examples_common_pth = \"./results/n-grams/wmt/pile/exp4/n_samples_None_fkeyFalse_rkeyFalse_fstopTrue_onlyalphaTrue/2/common/lang_dfs_filter_charsFalse_percentile0.999_n_gram2.pkl\"\n",
    "df = pickle.load(open(examples_common_pth, \"rb\"))\n",
    "df_12b = df['pythia-12b']\n",
    "df_fr_en = df_12b[df_12b['task'] == 'fr-en']\n",
    "df_fr_en.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([1, 56]), tokens: torch.Size([1, 37])\n",
      "Aligned logits shape: torch.Size([1, 37])\n",
      "logits: torch.Size([1, 56]), tokens: torch.Size([1, 37])\n",
      "Aligned logits shape: torch.Size([1, 37])\n",
      "logits: torch.Size([1, 92]), tokens: torch.Size([1, 69])\n",
      "Aligned logits shape: torch.Size([1, 69])\n",
      "logits: torch.Size([1, 92]), tokens: torch.Size([1, 69])\n",
      "Aligned logits shape: torch.Size([1, 69])\n",
      "logits: torch.Size([1, 92]), tokens: torch.Size([1, 69])\n",
      "Aligned logits shape: torch.Size([1, 69])\n",
      "logits: torch.Size([1, 92]), tokens: torch.Size([1, 69])\n",
      "Aligned logits shape: torch.Size([1, 69])\n",
      "logits: torch.Size([1, 92]), tokens: torch.Size([1, 69])\n",
      "Aligned logits shape: torch.Size([1, 69])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = 'EleutherAI/pythia-12b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "def test_logits(model_logits, df_fr_en, tokenizer):\n",
    "    for idx, row in df_fr_en.iterrows():\n",
    "        id_ = int(row['id'].split(\"_\")[0])  # row['id'] = id_task\n",
    "        logits = model_logits[id_]\n",
    "        gen_text = row['result']\n",
    "        \n",
    "        tokens = tokenizer(gen_text, return_tensors='pt', add_special_tokens=False)\n",
    "        \n",
    "        # Align logits with tokens, including special tokens\n",
    "        # This assumes that logits are for each token in the sequence, including special tokens.\n",
    "        aligned_logits = logits[:, :tokens['input_ids'].size(1)]\n",
    "        \n",
    "        # print(f\"id: {id_}, gen: {gen_text}\")\n",
    "        print(f\"logits: {logits.shape}, tokens: {tokens['input_ids'].shape}\")\n",
    "        print(f\"Aligned logits shape: {aligned_logits.shape}\")\n",
    "        \n",
    "        if idx > 5:\n",
    "            break\n",
    "\n",
    "test_logits(model_logits, df_fr_en, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probs_column(model_logits, df, tokenizer):\n",
    "    for idx, row in df_fr_en.iterrows():\n",
    "        id_ = int(row['id'].split(\"_\")[0])  # row['id'] = id_task\n",
    "        \n",
    "        probs = model_logits[id_].flatten()\n",
    "        gen_text = row['result']\n",
    "        \n",
    "        tokens = tokenizer(gen_text, return_tensors='pt', add_special_tokens=False).flatten()\n",
    "        len_tokens = len(tokens['input_ids'].size(1))\n",
    "        \n",
    "        # Align logits with tokens, including special tokens\n",
    "        # This assumes that logits are for each token in the \n",
    "        # sequence, including special tokens.\n",
    "        aligned_logits = logits[:len_tokens]\n",
    "        probs = probs[:len_tokens]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(f\"id: {id_}, gen: {gen_text}\")\n",
    "        print(f\"logits: {logits.shape}, tokens: {tokens['input_ids'].shape}\")\n",
    "        print(f\"Aligned logits shape: {aligned_logits.shape}\")\n",
    "        \n",
    "        if idx > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr_en['src'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts = df_fr_en.groupby('lang_1')['value'].sum()\n",
    "n_gram_distribution = ngram_counts / ngram_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = df_fr_en['results']\n",
    "ngram_combinations = wt.process_text(result_1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_ngrams_dist(df):\n",
    "    def get_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
