{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /local/home/antonis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /local/home/antonis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[dynet] random seed: 1234\n",
      "[dynet] allocating memory: 32MB\n",
      "[dynet] memory allocation done.\n",
      "initializing identifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "from src.wimbd_ import WimbdAnalysis, WimbdTasks, display_language_pairs\n",
    "\n",
    "bigram = \"bonjour\"  # Replace with your bigram\n",
    "language, _ = langid.classify(bigram)\n",
    "print(\"Detected language:\", language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pth = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/results/n-grams/wmt/pile/exp4/n_samples_None_fkeyFalse_rkeyFalse_fstopTrue_onlyalphaTrue/2/all/('cs', 'en').pkl\"\n",
    "# with open(pth, \"rb\") as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang params\n",
    "N_GRAMS = 1\n",
    "BASE_DIR = f\"./results/n-grams/wmt/pile/exp4/n_samples_None_fkeyFalse_rkeyFalse_fstopTrue_onlyalphaTrue\"\n",
    "# BASE_DIR = f\"./results/n-grams/exp_full/\"\n",
    "\n",
    "BASE_PATH = os.path.join(BASE_DIR, str(N_GRAMS))\n",
    "BASE_PATH_COMMON = os.path.join(BASE_PATH, \"common\")\n",
    "BASE_PATH_ALL = os.path.join(BASE_PATH, \"all\")\n",
    "FILTER_CHARS = False\n",
    "DETECT_LANG = False\n",
    "PERCENTILE = 0.1\n",
    "\n",
    "# model params\n",
    "base_results_path = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/pythia/experiment_3/inference/EleutherAI\"\n",
    "models = ['pythia-12b', 'pythia-6.9b', 'pythia-2.8b', 'pythia-1.4b', 'pythia-410m', 'pythia-160m', 'pythia-70m', 'pythia-31m', 'pythia-14m',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_freqs_path = \"./results/n-grams/exp_full/2/common/pl-en-2-grams.pkl\"\n",
    "n_gram_freqs = pickle.load(open(n_gram_freqs_path, \"rb\"))\n",
    "n_gram_freqs = dict(sorted(n_gram_freqs.items(), key=lambda item: item[1]['value'], reverse=True))\n",
    "# # export as text\n",
    "# # with open(\"n_gram_freqs.txt\", \"w\") as f:\n",
    "# #     for k, v in n_gr\n",
    "# # 3am_freqs.items():\n",
    "# #         f.write(f\"{k}: {v}\\n\")\n",
    "# LANGUAGES = ['ru-en', 'fr-en', 'ro-en', 'de-en', 'pl-en', 'cs-en']\n",
    "# LANGUAGES = ['ru-en', 'ro-en', 'de-en', 'pl-en', 'cs-en', 'fr-en', 'ja-en', 'zh-en']\n",
    "\n",
    "# wmt09\n",
    "LANGUAGES = ['wmt09-cs-en', 'wmt09-de-en', 'wmt09-fr-en', 'wmt09-es-en', 'wmt09-it-en', 'wmt09-hu-en']\n",
    "\n",
    "\n",
    "wa = WimbdAnalysis(BASE_PATH, LANGUAGES, N_GRAMS, FILTER_CHARS)\n",
    "wt = WimbdTasks()\n",
    "\n",
    "\n",
    "LANGUAGES_STR = \"_\".join(LANGUAGES)\n",
    "PLOT_PATH = f\"./results/n-grams/exp_full/{N_GRAMS}/plots/{LANGUAGES_STR}_filter_{FILTER_CHARS}\"\n",
    "if not os.path.exists(PLOT_PATH):\n",
    "    os.makedirs(PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results/n-grams/wmt/pile/exp4/n_samples_None_fkeyFalse_rkeyFalse_fstopTrue_onlyalphaTrue/1/common'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_PATH_COMMON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_dfs_all = wa.get_lang_dfs(BASE_PATH_ALL, LANGUAGES, filter_chars=False,\n",
    "#                                percentile=0.95)\n",
    "lang_dfs_common = wa.get_lang_dfs(BASE_PATH_COMMON, LANGUAGES, filter_chars=FILTER_CHARS,\n",
    "                                  percentile=0.95, n_gram=N_GRAMS)\n",
    "# lang_dfs_common_0 = wa.get_lang_dfs(BASE_PATH_COMMON, LANGUAGES, filter_chars=FILTER_CHARS,\n",
    "#                                     percentile=0.0)\n",
    "# lang_dfs_all_filtered = wa.get_lang_dfs(BASE_PATH_ALL, LANGUAGES, filter_chars=True,\n",
    "#                                         percentile=0.95, detect_lang=False, filter_entities=True)\n",
    "# lang_dfs_common_filtered = wa.get_lang_dfs(BASE_PATH_COMMON, LANGUAGES, filter_chars=False,\n",
    "#                                            percentile=0.999, detect_lang=False, filter_entities=True)\n",
    "lang_dfs = lang_dfs_common\n",
    "\n",
    "# get total samples per language pair\n",
    "lang_dfs_total_samples = {lang: df['value'].sum() for lang, df in lang_dfs.items()}\n",
    "colors = sns.color_palette('hls', len(lang_dfs))\n",
    "color_mapping = {lang: color for lang, color in zip(lang_dfs.keys(), colors)}\n",
    "\n",
    "# lang_dfs_common = lang_dfs_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dfs_new = \"./results/n-grams/exp_full/2/examples_dfs_0-shot_common_models.pkl\"\n",
    "model_instance_pth = \"./results/n-grams/exp_full/2/model_instance_results_0-shot_common.pkl\"\n",
    "lang_dfs_new = pickle.load(open(lang_dfs_new, \"rb\"))\n",
    "model_instance_results = pickle.load(open(model_instance_pth, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = lang_dfs_new['pythia-12b']\n",
    "examples.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results_from_df(df, metric='bleu'):\n",
    "    \"\"\"\"\n",
    "    returns a dict of structure {task: {model:{scores, nsamples}}\n",
    "    \"\"\"\n",
    "    model2score = {}\n",
    "    dataset2score = {}\n",
    "    for model, model_results in df.items():\n",
    "        for task in sorted(model_results['task'].unique()):\n",
    "            if model not in model2score:\n",
    "                model2score[model] = {'score': [], 'n_samples': []}\n",
    "            if task not in dataset2score:\n",
    "                dataset2score[task] = {'score': [], 'n_samples': []}\n",
    "            task_df = model_results[model_results['task'] == task]\n",
    "            task_score = task_df.iloc[0][metric]\n",
    "            task_n_samples = task_df['value'].sum()\n",
    "            model2score[model]['score'].append(task_score)\n",
    "            model2score[model]['n_samples'].append(task_n_samples)\n",
    "            dataset2score[task]['score'].append(task_score)\n",
    "            dataset2score[task]['n_samples'].append(task_n_samples)\n",
    "    return model2score, dataset2score\n",
    "\n",
    "def get_task_ngrams_from_df(df):\n",
    "    examples = df['pythia-12b'] # the ngram vals are same for all models\n",
    "    tasks = examples['task'].unique()\n",
    "    task_ngrams = {}\n",
    "    for task in tasks:\n",
    "        task_ngrams[task] = {'score': [], 'n_samples': []}\n",
    "        task_df = examples[examples['task'] == task]\n",
    "        score = task_df.iloc[0]['bleu']\n",
    "        # n_samples = \n",
    "        task_ngrams[task] = task_df\n",
    "    return task_ngrams\n",
    "\n",
    "\n",
    "model2score, dataset2score = get_model_results_from_df(lang_dfs_new)\n",
    "task_ngrams = get_task_ngrams_from_df(lang_dfs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model2score, dataset2score, color_mapping,\n",
    "               model_color_mapping, name=\"XY_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_new, dataset_scores_new = wa.prepare_scores(results_dict, lang_dfs_new, models,\n",
    "                                                         mutual_info=lang_mutual_info,\n",
    "                                                         merged_mutual_info=merged_mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f\"./results/n-grams/exp_full/{N_GRAMS}/common\"\n",
    "# # for each language, save top 100 examples in a csv\n",
    "\n",
    "# for lang, df in lang_dfs.items():\n",
    "#     df = df.sort_values(by=['value'], ascending=False)\n",
    "#     df.to_csv(os.path.join(save_path, f\"{lang}_samples.csv\"), index=False)\n",
    "\n",
    "# for lang, df in lang_dfs_filtered.items():\n",
    "#     df = df.sort_values(by=['value'], ascending=False)\n",
    "#     df = df.head(100)\n",
    "#     df.to_csv(os.path.join(save_path, f\"{lang}_samples_filtered.csv\"), index=False)\n",
    "\n",
    "# n_gram_freqs_all = wa.get_lang_dfs(BASE_PATH_ALL, datasets=LANGUAGES, is_all=True, filter_chars=FILTER_CHARS)\n",
    "# print(n_gram_freqs_all.keys())\n",
    "\n",
    "# lang_dfs_all = wa.load_all(\"all.pkl.csv\")\n",
    "# df_langs = pd.concat(lang_dfs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gram_freqs_all = wa.get_lang_dfs(BASE_PATH_ALL, datasets=LANGUAGES, is_all=True, filter_chars=FILTER_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lang, df in lang_dfs_all.items():\n",
    "#     print(f\"{lang}: {len(df)}\")\n",
    "\n",
    "# # display top 100 of each language\n",
    "# for lang, df in lang_dfs.items():\n",
    "#     print(f\"-------- lang: {lang} --------\")\n",
    "#     df = df.sort_values(by=['value'], ascending=False)\n",
    "#     print(f\"// top 100\")\n",
    "#     display(df.iloc[:100])\n",
    "#     midpoint = np.median(df['value'])\n",
    "#     print(f\"// mid 100\")\n",
    "#     display(df[df['value'] <= midpoint].iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gram_freqs.pop(('angela merkel', 'chancellor angela'))\n",
    "n_gram_freqs_values = [v['value'] for v in n_gram_freqs.values()]\n",
    "n_gram_freqs_values_cumsum = np.cumsum(n_gram_freqs_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "wa.plot_n_samples(lang_dfs_common, color_mapping)\n",
    "# wa.plot_n_samples(lang_dfs_filtered, color_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_colormap = plt.cm.get_cmap('coolwarm', len(models))\n",
    "model_color_mapping = {model: model_colormap(1 - i / len(models)) for i, model in enumerate(models)}\n",
    "\n",
    "\n",
    "# wa.analyze_and_plot_distributions(lang_dfs_all)\n",
    "wa.analyze_and_plot_distributions(lang_dfs_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the contents of results.json for each model/dataset pair\n",
    "results_dict = collections.defaultdict(dict)\n",
    "\n",
    "# Iterate over each model and dataset, loading the results.json file\n",
    "for model in models:\n",
    "    for lang_dataset in LANGUAGES:\n",
    "        lang_pair = '-'.join(lang_dataset.split('-')[-2:])\n",
    "        results_path = os.path.join(base_results_path, model, \"TRANSLATION\", lang_dataset, '0-shot', 'results.json')\n",
    "        \n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                results_dict[lang_pair][model] = data['results'][lang_dataset]['bleu']\n",
    "        else:\n",
    "            print(f\"File does not exist: {results_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in lang_dfs.items():\n",
    "    print(f\"key: {key}, lang_df_task: {df['task'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_bleu_scores(results_dict, models, color_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_ds = wa.build_lang_ds(lang_dfs.keys())\n",
    "\n",
    "lang_mutual_info, merged_mutual_info = wa.calculate_mutual_info(lang_dfs, lang_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_mutual_info, merged_mutual_info = wa.calculate_mutual_info(lang_dfs, lang_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores, dataset_scores = wa.prepare_scores(results_dict, lang_dfs, models, \n",
    "                                                 mutual_info=lang_mutual_info,\n",
    "                                                 merged_mutual_info=merged_mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# wa.plot_scores(model_scores, dataset_scores, model_color_mapping, name=\"XY_pairs\")\n",
    "wa.plot_scores(model_scores, dataset_scores, color_mapping,\n",
    "               model_color_mapping, name=\"XY_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores, dataset_scores, color_mapping,\n",
    "            model_color_mapping, name=\"XY_pairs\",\n",
    "            x_key='coverage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores, dataset_scores, color_mapping,\n",
    "            model_color_mapping, name=\"XY_pairs\",\n",
    "            x_key='coverage', fit_polynomial=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_common_0, dataset_scores_common_0 = wa.prepare_scores(results_dict, lang_dfs_common_0, models, \n",
    "                                                 mutual_info=lang_mutual_info,\n",
    "                                                 merged_mutual_info=merged_mutual_info)\n",
    "wa.plot_scores(model_scores_common_0, dataset_scores_common_0, color_mapping,\n",
    "            model_color_mapping, name=\"XY_pairs\",\n",
    "            x_key='coverage', fit_polynomial=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_all, dataset_scores_all = wa.prepare_scores(results_dict, lang_dfs_all, models,\n",
    "                                                         mutual_info=lang_mutual_info,\n",
    "                                                         merged_mutual_info=merged_mutual_info)\n",
    "wa.plot_scores(model_scores_all, dataset_scores_all, color_mapping,\n",
    "                model_color_mapping, name=\"All\")\n",
    "\n",
    "wa.plot_scores(model_scores_all, dataset_scores_all, color_mapping,\n",
    "                model_color_mapping, name=\"All\",\n",
    "                x_key='coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_filered, dataset_scores_filtered = wa.prepare_scores(results_dict, lang_dfs_common_filtered, models)\n",
    "wa.plot_scores(model_scores_filered, dataset_scores_filtered, \n",
    "            color_mapping, model_color_mapping,\n",
    "            name=\"XY_pairs_filtered\", x_key='coverage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores, dataset_scores, \n",
    "            color_mapping, model_color_mapping,\n",
    "            name=\"XY_pairs\",\n",
    "            x_key=\"mutual_info\")\n",
    "\n",
    "wa.plot_scores(model_scores, dataset_scores, \n",
    "            color_mapping, model_color_mapping,\n",
    "            name=\"XY_pairs\",\n",
    "            x_key=\"merged_mutual_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_common, dataset_scores_common = wa.prepare_scores(results_dict, lang_dfs_common, models)\n",
    "wa.plot_scores(model_scores_common, dataset_scores_common, \n",
    "            color_mapping, model_color_mapping,\n",
    "            name=\"XY_pairs_common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average line over all models for all datasets\n",
    "avg_dataset_scores = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "avg_scores = {'score': [], 'n_samples': []} \n",
    "for dataset in dataset_scores:\n",
    "    avg_dataset_scores[dataset]['score'] = np.mean(dataset_scores[dataset]['score'])\n",
    "    avg_dataset_scores[dataset]['n_samples'] = np.mean(dataset_scores[dataset]['n_samples'])\n",
    "    plt.scatter(avg_dataset_scores[dataset]['n_samples'], avg_dataset_scores[dataset]['score'], \n",
    "                color=color_mapping[dataset], label=dataset)\n",
    "    avg_scores['score'].append(avg_dataset_scores[dataset]['score'])\n",
    "    avg_scores['n_samples'].append(avg_dataset_scores[dataset]['n_samples'])\n",
    "\n",
    "# sort by n_samples\n",
    "avg_scores['score'] = [x for _, x in sorted(zip(avg_scores['n_samples'], avg_scores['score']))]\n",
    "avg_scores['n_samples'] = sorted(avg_scores['n_samples'])\n",
    "\n",
    "plt.plot(avg_scores['n_samples'], avg_scores['score'], color='black', label='average')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['fr-en'].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['pythia-12b', 'pythia-6.9b', 'pythia-2.8b', 'pythia-1.4b', 'pythia-410m', 'pythia-160m', 'pythia-70m', 'pythia-31m', 'pythia-14m']\n",
    "\n",
    "model_param_map = {'pythia-12b': 12e09,\n",
    "                    'pythia-6.9b': 6.9e09,\n",
    "                    'pythia-2.8b': 2.8e09,\n",
    "                    'pythia-1.4b': 1.4e09,\n",
    "                    'pythia-410m': 410e06,\n",
    "                    'pythia-160m': 160e06,\n",
    "                    'pythia-70m': 70e06,\n",
    "                    'pythia-31m': 31e06,\n",
    "                    'pythia-14m': 14e06,}\n",
    "\n",
    "wa.plot_model_size_vs_scores(results_dict, models, model_param_map, color_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total samples per language\n",
    "lang_dfs_all_total_samples = {}\n",
    "for lang, df in lang_dfs_all.items():\n",
    "    lang_dfs_all_total_samples[lang] = df['value'].sum()\n",
    "\n",
    "model_scores_all, dataset_scores_all = wa.prepare_scores(results_dict, lang_dfs_all, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gram_freqs_all['ja-en']['value'] = pd.to_numeric(n_gram_freqs_all['ja-en']['value'], errors='coerce')\n",
    "# largest_values = n_gram_freqs_all['ja-en'].nlargest(50, columns=['value'])\n",
    "\n",
    "# print(largest_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=f\"all instances NGRAM={N_GRAMS}\")\n",
    "\n",
    "# wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "#             color_mapping, model_color_mapping, \n",
    "#             name=f\"all instances NGRAM={N_GRAMS}\",\n",
    "#             x_key=\"coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores, dataset_scores, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=\"XY_pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dfs_all.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'fr'\n",
    "\n",
    "for lang_pair in lang_dfs_all:\n",
    "    print(f\"lang_pair: {lang_pair}\")\n",
    "    lang = lang_pair.split('-')[0]\n",
    "    print(f\"lang: {lang}\")\n",
    "    lang_en_all = lang_dfs_all[f'{lang}-en']\n",
    "    lang_only = lang_en_all[lang_en_all['lang'] == lang]\n",
    "    print(f\"lang all: {len(lang_en_all)} only: {len(lang_only)} diff: {len(lang_en_all) - len(lang_only)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'fr'\n",
    "\n",
    "for lang_pair in lang_dfs:\n",
    "    lang = lang_pair.split('-')[0]\n",
    "    lang_en_all = lang_dfs_all[f'{lang}-en']\n",
    "    lang_only = lang_en_all[lang_en_all['lang'] == lang]\n",
    "    print(f\"lang all: {len(lang_en_all)} only: {len(lang_only)} diff: {len(lang_en_all) - len(lang_only)}\")\n",
    "\n",
    "lang_dfs_all_single = {lang_pair: df[df['lang'] == lang_pair.split('-')[0]] for lang_pair, df in lang_dfs_all.items()}\n",
    "model_scores_all_single, dataset_scores_all_single = wa.prepare_scores(results_dict, lang_dfs_all_single, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores_all_single, dataset_scores_all_single, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=\"all single instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
