{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/antonis/.conda/envs/incidental/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-22 06:30:18.710022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/pythia/experiment_1/huggingface/flan_v1/c4_mixed_NLI/EleutherAI/pythia-160M-deduped_ckpt_False/checkpoint-70000\"\n",
    "dataset_path = '/share/edc/home/antonis/datasets/huggingface/flan_v1_task_ds_n_5000'\n",
    "device = \"cpu\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from datasets import load_from_disk\n",
    "\n",
    "set_seed(420)\n",
    "text_column_name = \"text\"\n",
    "def tokenize_function(examples):\n",
    "        with CaptureLogger(tok_logger) as cl:\n",
    "            output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "                \" before being passed to the model.\"\n",
    "            )\n",
    "        return output\n",
    "\n",
    "def load_model(model_name_or_path, device=device):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, config=config)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, config=config)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return config, tokenizer, model\n",
    "\n",
    "_, tokenizer, model = load_model(model_name_or_path)\n",
    "\n",
    "raw_dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_2_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m raw_dataset:\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_dataset, datasets\u001b[39m.\u001b[39mdataset_dict\u001b[39m.\u001b[39mDatasetDict):\n\u001b[0;32m---> 11\u001b[0m         pth \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_2_path, x)\n\u001b[1;32m     12\u001b[0m         \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(pth), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m not found at \u001b[39m\u001b[39m{\u001b[39;00mpth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         tokenized_datasets[x] \u001b[39m=\u001b[39m load_from_disk(pth)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_2_path' is not defined"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for x in raw_dataset:\n",
    "    if isinstance(raw_dataset, datasets.dataset_dict.DatasetDict):\n",
    "        pth = os.path.join(dataset_2_path, x)\n",
    "        assert os.path.exists(pth), f\"Dataset {x} not found at {pth}\"\n",
    "        tokenized_datasets[x] = load_from_disk(pth)\n",
    "    else:\n",
    "        tokenized_datasets = load_from_disk(dataset_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenized_datasets['NLI'][0]['inputs']\n",
    "# inputs = tokenizer(inputs, return_tensors='pt')\n",
    "\n",
    "# # Shift the input_ids one token to the right to create the labels\n",
    "# labels = inputs['input_ids'].clone()\n",
    "# labels[:-1] = inputs['input_ids'][1:]\n",
    "\n",
    "# # Calculate the loss\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "import torch.nn as nn\n",
    "\n",
    "# Convert your model to a functional model\n",
    "fmodel, params, buffers = make_functional_with_buffers(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a function to compute the loss of the model given a single input\n",
    "def compute_loss_stateless_model(params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    outputs = fmodel(params, buffers, batch)\n",
    "    loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
    "    return loss\n",
    "\n",
    "# Create a new function that computes the gradient with respect to the params\n",
    "ft_compute_grad = grad(compute_loss_stateless_model)\n",
    "\n",
    "# Use vmap to get the function to compute the gradient over an entire batch of samples and targets\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "\n",
    "\n",
    "# Extract the input_ids tensor from the BatchEncoding object\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Compute per-sample-gradients\n",
    "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
