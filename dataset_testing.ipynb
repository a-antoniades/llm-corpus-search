{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from promptsource import templates\n",
    "\n",
    "CACHE_DIR = \"/share/edc/home/antonis/datasets/huggingface\"\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = CACHE_DIR\n",
    "\n",
    "# # Get a list of all supported datasets\n",
    "# datasets = templates.get_dataset_names()\n",
    "# print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dataset_type': 'QA', 'dataset_name': 'common_gen', 'dataset_config_name': 'common_gen', 'p': 0.5, 'train_split': 'train'}, {'dataset_type': 'QA', 'dataset_name': 'e2e_nlg', 'dataset_config_name': 'e2e_nlg', 'p': 0.5, 'train_split': 'train'}, {'dataset_type': 'QA', 'dataset_name': 'dart', 'dataset_config_name': 'dart', 'p': 0.5, 'validation_split': 'validation'}, {'dataset_type': 'QA', 'dataset_name': 'web_nlg', 'dataset_config_name': 'release_v3.0_en', 'p': 0.5, 'validation_split': 'test'}, {'dataset_type': 'text', 'dataset_name': 'wikitext', 'dataset_config_name': 'wikitext-2-v1', 'p': 1, 'train_split': 'train'}, {'dataset_type': 'text', 'dataset_name': 'bookcorpus', 'dataset_config_name': None, 'p': 1, 'train_split': 'train'}]\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/src\")\n",
    "from src.dataset_configs import DatasetConfig\n",
    "\n",
    "config_instance = DatasetConfig()\n",
    "\n",
    "# Changing P_QA and updating 'p' values in dataset_configs\n",
    "config_instance.update_p_values(new_value=0.5)\n",
    "\n",
    "# Now, the 'p' values inside config_instance.dataset_configs are updated\n",
    "print(config_instance.dataset_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pth = \"/share/edc/home/antonis/datasets/huggingface/merged_datasets/dataset_1/dataset_validation.arrow\"\n",
    "dataset = load_from_disk(ds_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"tripleset: [['Mars Hill College', 'JOINED', '1973'], ['Mars Hill College', 'LOCATION', 'Mars Hill, North Carolina']] annotations: 'source': ['WikiSQL_decl_sents'], 'text': ['A school from Mars Hill, North Carolina, joined in 1973.']\",\n",
       " 'tripleset: [[\\'Newberry College\\', \\'NICKNAME\\', \\'Wolves\\']] annotations: \\'source\\': [\\'WikiSQL_decl_sents\\'], \\'text\\': [\"Newberry College\\'s nickname is the wolves.\"]',\n",
       " \"tripleset: [['Presbyterian College', 'TYPE', 'Private']] annotations: 'source': ['WikiSQL_decl_sents'], 'text': ['Presbyterian College is a private school.']\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Create a concise and grammatically correct sentence that incorporates the information provided in the triple set. Please ensure that your sentence naturally integrates this information:\n",
      "- Mars Hill College JOINED 1973\n",
      "- Mars Hill College LOCATION Mars Hill, North Carolina\n",
      "Make sure your sentence reads naturally and is informative.\n",
      "Answer: 'A school from Mars Hill, North Carolina, joined in 1973.'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def generate_qa_prompt(example):\n",
    "    # Extract the tripleset from the input example\n",
    "    tripleset_match = re.search(\"tripleset: (.+?) annotations\", example)\n",
    "    if tripleset_match:\n",
    "        tripleset_str = tripleset_match.group(1)\n",
    "        tripleset = json.loads(tripleset_str.replace(\"\\'\", \"\\\"\"))\n",
    "    else:\n",
    "        return \"Error: Could not extract tripleset from example.\"\n",
    "    \n",
    "    # Extract the text (answer) from the input example\n",
    "    text_match = re.search(\"text': \\[(.+?)]\", example)\n",
    "    if text_match:\n",
    "        text_str = text_match.group(1)\n",
    "        # replace escaped single quotes with actual single quotes\n",
    "        answer = text_str.replace(\"\\\\'\", \"'\")\n",
    "    else:\n",
    "        return \"Error: Could not extract text from example.\"\n",
    "    \n",
    "    # Construct the prompt using the information in the tripleset\n",
    "    info_list = [\"- {} {} {}\".format(triple[0], triple[1], triple[2]) for triple in tripleset]\n",
    "    info_text = \"\\n\".join(info_list)\n",
    "    prompt = (\"Create a concise and grammatically correct sentence that \"\n",
    "              \"incorporates the information provided in the triple set. \"\n",
    "              \"Please ensure that your sentence naturally integrates this \"\n",
    "              \"information:\\n{}\\nMake sure your sentence reads naturally and is informative.\"\n",
    "              .format(info_text))\n",
    "    \n",
    "    return {'prompt': prompt, 'answer': answer}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "example = \"tripleset: [['Mars Hill College', 'JOINED', '1973'], ['Mars Hill College', 'LOCATION', 'Mars Hill, North Carolina']] annotations: 'source': ['WikiSQL_decl_sents'], 'text': ['A school from Mars Hill, North Carolina, joined in 1973.']\"\n",
    "\n",
    "result = generate_qa_prompt(example)\n",
    "print(\"Prompt:\", result['prompt'])\n",
    "print(\"Answer:\", result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Please formulate a sentence using the information provided below. Ensure it is grammatically correct and makes logical sense:\n",
      "- Mars Hill College JOINED 1973\n",
      "- Mars Hill College LOCATION Mars Hill, North Carolina\n",
      "Answer: 'A school from Mars Hill, North Carolina, joined in 1973.'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "def generate_qa_prompt(example):\n",
    "    # Extract the tripleset from the input example\n",
    "    tripleset_match = re.search(\"tripleset: (.+?) annotations\", example)\n",
    "    if tripleset_match:\n",
    "        tripleset_str = tripleset_match.group(1)\n",
    "        tripleset = json.loads(tripleset_str.replace(\"\\'\", \"\\\"\"))\n",
    "    else:\n",
    "        return \"Error: Could not extract tripleset from example.\"\n",
    "    \n",
    "    # Extract the text (answer) from the input example\n",
    "    text_match = re.search(\"text': \\[(.+?)]\", example)\n",
    "    if text_match:\n",
    "        text_str = text_match.group(1)\n",
    "        # replace escaped single quotes with actual single quotes\n",
    "        answer = text_str.replace(\"\\\\'\", \"'\")\n",
    "    else:\n",
    "        return \"Error: Could not extract text from example.\"\n",
    "    \n",
    "    # Construct the info list\n",
    "    info_list = [\"- {} {} {}\".format(triple[0], triple[1], triple[2]) for triple in tripleset]\n",
    "    info_text = \"\\n\".join(info_list)\n",
    "    \n",
    "    # Define 10 different prompt structures\n",
    "    prompt_structures = [\n",
    "        \"Create a concise and grammatically correct sentence that incorporates the information provided in the triple set. Please ensure that your sentence naturally integrates this information:\\n{}\\nMake sure your sentence reads naturally and is informative.\",\n",
    "        \"Compose a short, grammatically accurate sentence that seamlessly includes the following details:\\n{}\\nEnsure the sentence flows naturally.\",\n",
    "        \"Using the following information, write a clear and concise sentence:\\n{}\\nThe sentence should be grammatically correct and easy to understand.\",\n",
    "        \"Construct a sentence using the information given below. Your sentence should be brief and grammatically correct:\\n{}\",\n",
    "        \"Your task is to create a sentence that communicates the information below in a clear and natural way:\\n{}\",\n",
    "        \"Write a grammatical sentence that integrates the following information in a coherent manner:\\n{}\",\n",
    "        \"Please formulate a sentence using the information provided below. Ensure it is grammatically correct and makes logical sense:\\n{}\",\n",
    "        \"Using the data points below, create a single sentence that is grammatically correct and effectively communicates the information:\\n{}\",\n",
    "        \"Construct a grammatically accurate and informative sentence using the details given below:\\n{}\",\n",
    "        \"Combine the information provided into a single, grammatically correct sentence that reads naturally:\\n{}\"\n",
    "    ]\n",
    "    \n",
    "    # Randomly select a prompt structure\n",
    "    prompt = random.choice(prompt_structures).format(info_text)\n",
    "    \n",
    "    return {'prompt': prompt, 'answer': answer}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "example = \"tripleset: [['Mars Hill College', 'JOINED', '1973'], ['Mars Hill College', 'LOCATION', 'Mars Hill, North Carolina']] annotations: 'source': ['WikiSQL_decl_sents'], 'text': ['A school from Mars Hill, North Carolina, joined in 1973.']\"\n",
    "\n",
    "result = generate_qa_prompt(example)\n",
    "print(\"Prompt:\", result['prompt'])\n",
    "print(\"Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 11.5k/11.5k [00:00<00:00, 10.8MB/s]\n",
      "Downloading metadata: 100%|██████████| 31.7k/31.7k [00:00<00:00, 28.1MB/s]\n",
      "Downloading readme: 100%|██████████| 22.1k/22.1k [00:00<00:00, 21.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset web_nlg/release_v3.0_en to /local/home/antonis/.cache/huggingface/datasets/web_nlg/release_v3.0_en/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 25.5MB [00:00, 65.6MB/s]\n",
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset web_nlg downloaded and prepared to /local/home/antonis/.cache/huggingface/datasets/web_nlg/release_v3.0_en/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 575.32it/s]\n"
     ]
    }
   ],
   "source": [
    "ds_dart = load_dataset('web_nlg', 'release_v3.0_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['category', 'size', 'eid', 'original_triple_sets', 'modified_triple_sets', 'shape', 'shape_type', 'lex', 'test_category', 'dbpedia_links', 'links'],\n",
       "        num_rows: 13211\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['category', 'size', 'eid', 'original_triple_sets', 'modified_triple_sets', 'shape', 'shape_type', 'lex', 'test_category', 'dbpedia_links', 'links'],\n",
       "        num_rows: 1667\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['category', 'size', 'eid', 'original_triple_sets', 'modified_triple_sets', 'shape', 'shape_type', 'lex', 'test_category', 'dbpedia_links', 'links'],\n",
       "        num_rows: 5713\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = [\n",
    "    \"/share/edc/home/antonis/datasets/huggingface/merged_datasets/dataset_1/dataset_train.arrow\",\n",
    "    \"/share/edc/home/antonis/datasets/huggingface/merged_datasets/dataset_0/dataset_train.arrow\",\n",
    "]\n",
    "\n",
    "for ds_file in dataset_files:\n",
    "    ds = load_from_disk(ds_file)\n",
    "    print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    print(\"preds1, \", preds.shape)\n",
    "    print(\"labels1, \", labels.shape)\n",
    "    decoded_preds, decoded_labels = tokenizer.batch_decode((preds, labels), skip_special_tokens=True)\n",
    "    labels = labels[:, 1:].tolist()\n",
    "    preds = preds[:, :-1].tolist()\n",
    "\n",
    "    print(f\"labels: {labels.shape}\")\n",
    "    print(f\"preds: {preds.shape}\")\n",
    "    print(f\"decoded_labels: {decoded_labels.shape}\")\n",
    "    print(f\"decoded_preds: {decoded_preds.shape}\")\n",
    "\n",
    "    metric1 = evaluate.load(\"accuracy\")\n",
    "    metric2 = evaluate.load(\"f1\")\n",
    "    metric3 = evaluate.load(\"bleu\")\n",
    "    metric4 = evaluate.load(\"bertscore\")\n",
    "\n",
    "    accuracy, f1 = [], []\n",
    "    for i in range(len(labels)):\n",
    "        accuracy.append(metric1.compute(predictions=preds[i], references=labels[i]))\n",
    "        f1.append(metric2.compute(predictions=preds[i], references=labels[i], average='macro'))\n",
    "    accuracy = np.mean(accuracy)\n",
    "    f1 = np.mean(f1)\n",
    "    # Specifying the average method for multiclass F1 score\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"bleu\": metric3.compute(predictions=preds, references=labels),\n",
    "        \"bertscore\": metric4.compute(predictions=preds, references=labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset[1])\n",
    "idx = 1\n",
    "for k, v in dataset[idx].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conf_1 = dataset_configs[1]\n",
    "ds_1 = load_dataset(ds_conf_1['dataset_name'], ds_conf_1['dataset_config_name'], cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conf_0 = dataset_configs[0]\n",
    "ds_0 = load_dataset(ds_conf_0['dataset_name'], ds_conf_0['dataset_config_name'], cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merge_datasets import concatenate_columns\n",
    "\n",
    "ds_0_train = ds_0['train']\n",
    "ds_0_text = ds_0_train.map(lambda x: concatenate_columns(x, new_col_name='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conf_2 = dataset_configs[2]\n",
    "ds_2 = load_dataset(ds_conf_2['dataset_name'], ds_conf_2['dataset_config_name'], cache_dir=CACHE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
