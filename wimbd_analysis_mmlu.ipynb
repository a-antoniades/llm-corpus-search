{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /local/home/antonis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /local/home/antonis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[dynet] random seed: 1234\n",
      "[dynet] allocating memory: 32MB\n",
      "[dynet] memory allocation done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "import langid\n",
    "from src.wimbd_ import WimbdAnalysis, load_mmlu, DataConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['professional_psychology', 'professional_law']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataConfigs.mmlu_tasks['professional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_freqs_path = \"./results/n-grams/exp_full/2/common/pl-en-2-grams.pkl\"\n",
    "n_gram_freqs = pickle.load(open(n_gram_freqs_path, \"rb\"))\n",
    "n_gram_freqs = dict(sorted(n_gram_freqs.items(), key=lambda item: item[1]['value'], reverse=True))\n",
    "# # export as text\n",
    "# # with open(\"n_gram_freqs.txt\", \"w\") as f:\n",
    "# #     for k, v in n_gr\n",
    "# # 3am_freqs.items():\n",
    "# #         f.write(f\"{k}: {v}\\n\")\n",
    "# LANGUAGES = ['ru-en', 'fr-en', 'ro-en', 'de-en', 'pl-en', 'cs-en']\n",
    "# LANGUAGES =  ['ru-en', 'ro-en', 'de-en', 'pl-en', 'cs-en', 'fr-en', 'ja-en', 'zh-en']\n",
    "\n",
    "# wmt09\n",
    "TASKS = DataConfigs.mmlu_tasks['professional']\n",
    "ds = load_mmlu(TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang params\n",
    "N_GRAMS = 5\n",
    "# BASE_DIR = \"./results/n-grams/mmlu/test-set/exp_full_None\"\n",
    "BASE_DIR = \"./results/n-grams/mmlu/exp3/test-set/exp_full_None\"\n",
    "# BASE_DIR = \"./results/n-grams/mmlu/pile/exp4_filter/test-set/exp_full_None\"\n",
    "# BASE_DIR = \"./results/n-grams/mmlu/pile/exp4_nofilter/test-set/exp_full_None\"\n",
    "BASE_PATH = os.path.join(BASE_DIR, f\"{N_GRAMS}\")\n",
    "BASE_PATH_COMMON = os.path.join(BASE_PATH, \"common\")\n",
    "BASE_PATH_ALL = os.path.join(BASE_PATH, \"all\")\n",
    "FILTER_CHARS = False\n",
    "DETECT_LANG = False\n",
    "LOG_AXIS = True\n",
    "TASK = \"MMLU/hendrycks*\"\n",
    "SHOT = \"0-shot\"\n",
    "\n",
    "# model params\n",
    "base_results_paths = {\n",
    "    \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/pythia/experiment_4/inference/EleutherAI\": [\n",
    "        'pythia-12b', 'pythia-6.9b', 'pythia-2.8b', \n",
    "        'pythia-1.4b', 'pythia-410m', 'pythia-160m', \n",
    "        'pythia-70m', 'pythia-31m', 'pythia-14m'\n",
    "    ],\n",
    "    \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/OLMO\": [\n",
    "        'OLMo-7b'\n",
    "    ]\n",
    "}\n",
    "\n",
    "models = ['pythia-12b', 'pythia-6.9b', 'pythia-2.8b', \n",
    "          'pythia-1.4b', 'pythia-410m', 'pythia-160m', \n",
    "          'pythia-70m', 'pythia-31m', 'pythia-14m',\n",
    "          'OLMo-7b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS_OMMIT = [\"formal_logic\"]\n",
    "# TASKS = [t for t in list(ds.keys()) if t not in TASKS_OMMIT]\n",
    "TASKS = MMLU_TASKS = DataConfigs.mmlu_tasks['professional']\n",
    "wa = WimbdAnalysis(BASE_PATH, TASKS, N_GRAMS, FILTER_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs, task_dfs_path = wa.get_task_dfs(BASE_PATH_COMMON, TASKS, filter_chars=FILTER_CHARS)\n",
    "task_dfs_filtered, task_dfs_filtered_path = wa.get_task_dfs(BASE_PATH, TASKS, filter_chars=True)\n",
    "# task_dfs\n",
    "\n",
    "# get total samples per language pair\n",
    "task_dfs_total_samples = {lang: df['value'].sum() for lang, df in task_dfs.items()}\n",
    "colors = sns.color_palette('hls', len(task_dfs))\n",
    "color_mapping = {lang: color for lang, color in zip(task_dfs.keys(), colors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = {task: ds[task] for task in task_dfs.keys()}\n",
    "# TASKS = [t for t in list(ds.keys()) if t not in TASKS_OMMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "for task in TASKS:\n",
    "    total_samples += task_dfs_total_samples[task]\n",
    "print(f\"Total samples: {total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import AnalyzeNgrams as an\n",
    "\n",
    "coverage_path_common = os.path.join(BASE_PATH_COMMON, \"task-coverage.pkl\")\n",
    "coverage_path_all = os.path.join(BASE_PATH_ALL, \"task-coverage.pkl\")\n",
    "\n",
    "task_cov_all, task_cov_all_mean = an.calculate_average_task_coverage(BASE_PATH_ALL, TASKS, [N_GRAMS])\n",
    "task_cov_common, task_cov_common_mean = an.calculate_average_task_coverage(BASE_PATH_COMMON, TASKS, [N_GRAMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs_all, task_dfs_all_path = wa.get_task_dfs(BASE_PATH_ALL, TASKS)\n",
    "task_dfs_common, task_dfs_common_path = wa.get_task_dfs(BASE_PATH_COMMON, TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export as .pkl\n",
    "# # save all tasks .pkl\n",
    "\n",
    "# with open(os.path.join(BASE_PATH, \"task_dfs_all.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(task_dfs_all, f)\n",
    "\n",
    "# with open(os.path.join(BASE_PATH, \"task_dfs_common.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(task_dfs_common, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_dfs_common['college_mathematics']\n",
    "# task_dfs_common['college_mathematics']['example'].iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/results/n-grams/mmlu/exp1/4/common/task-coverage.pkl\"\n",
    "\n",
    "with open(pth, \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f\"./results/n-grams/exp_full/{N_GRAMS}/common\"\n",
    "# # for each language, save top 100 examples in a csv\n",
    "\n",
    "# for lang, df in lang_dfs.items():\n",
    "#     df = df.sort_values(by=['value'], ascending=False)\n",
    "#     df.to_csv(os.path.join(save_path, f\"{lang}_samples.csv\"), index=False)\n",
    "\n",
    "# for lang, df in lang_dfs_filtered.items():\n",
    "#     df = df.sort_values(by=['value'], ascending=False)\n",
    "#     df = df.head(100)\n",
    "#     df.to_csv(os.path.join(save_path, f\"{lang}_samples_filtered.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gram_freqs.pop(('angela merkel', 'chancellor angela'))\n",
    "n_gram_freqs_values = [v['value'] for v in n_gram_freqs.values()]\n",
    "n_gram_freqs_values_cumsum = np.cumsum(n_gram_freqs_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage:\n",
    "# wa.plot_n_samples(task_dfs, color_mapping)\n",
    "# # wa.plot_n_samples(lang_dfs_filtered, color_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_colormap = plt.cm.get_cmap('coolwarm', len(models))\n",
    "model_color_mapping = {model: model_colormap(1 - i / len(models)) for i, model in enumerate(models)}\n",
    "\n",
    "\n",
    "# wa.analyze_and_plot_distributions(task_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the contents of results.json for each model/dataset pair\n",
    "results_dict = collections.defaultdict(dict)\n",
    "instance_results_dict = collections.defaultdict(dict)\n",
    "\n",
    "# Iterate over each model and dataset, loading the results.json file\n",
    "\n",
    "for base_results_path, models_ in base_results_paths.items():\n",
    "    for model in models_:\n",
    "        results_path = os.path.join(base_results_path, model, TASK, SHOT, 'results.json')\n",
    "        instance_results_path = os.path.join(base_results_path, model, TASK, SHOT, 'doc_results.json')\n",
    "        results = json.load(open(results_path, 'r'))['results']\n",
    "        doc_results = json.load(open(instance_results_path, 'r'))\n",
    "        for task in results.keys():\n",
    "            # remove hendrycksTest-\n",
    "            task_str = task.split(\"-\")[1]\n",
    "            if task_str in TASKS:\n",
    "                results_dict[task_str][model] = results[task]['acc']\n",
    "                if os.path.exists(instance_results_path):\n",
    "                    instance_results_dict[model][task_str] = doc_results[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_bleu_scores(results_dict, models, color_mapping)\n",
    "\n",
    "task_ds = ds\n",
    "\n",
    "# task_mutual_info, merged_mutual_info = wa.calculate_mutual_info(task_dfs, task_ds)\n",
    "task_mutual_info, merged_mutual_info = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores, dataset_scores = wa.prepare_scores(results_dict, task_dfs, \n",
    "                                                 models, coverage_=task_cov_common_mean,\n",
    "                                                 cov_mean=task_cov_common_mean)\n",
    "\n",
    "single_model = [\"pythia-12b\"]\n",
    "single_model_score, single_model_dataset_score = wa.prepare_scores(results_dict, task_dfs, \n",
    "                                                                   single_model, coverage_=task_cov_common_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_scores(dataset_scores, model_scores, color_mapping, \n",
    "                       model_list=None, x_key='n_samples',\n",
    "                       log_axis=False, title=None, annotate=False):\n",
    "    avg_dataset_scores = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "    avg_scores = {'score': [], x_key: []} \n",
    "\n",
    "    for dataset in dataset_scores:\n",
    "        if model_list is None:\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean(dataset_scores[dataset]['score'])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean(dataset_scores[dataset][x_key])\n",
    "        else:\n",
    "            model_indices = [i for i, model in enumerate(model_scores.keys()) if model in model_list]\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean([dataset_scores[dataset]['score'][i] for i in model_indices])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean([dataset_scores[dataset][x_key][i] for i in model_indices])\n",
    "\n",
    "        plt.scatter(avg_dataset_scores[dataset][x_key], avg_dataset_scores[dataset]['score'], \n",
    "                    color=color_mapping[dataset], label=dataset)\n",
    "        if annotate:\n",
    "            plt.annotate(dataset, (avg_dataset_scores[dataset][x_key], avg_dataset_scores[dataset]['score']))\n",
    "        avg_scores['score'].append(avg_dataset_scores[dataset]['score'])\n",
    "        avg_scores[x_key].append(avg_dataset_scores[dataset][x_key])\n",
    "\n",
    "    # sort by n_samples\n",
    "    avg_scores['score'] = [x for _, x in sorted(zip(avg_scores[x_key], avg_scores['score']))]\n",
    "    avg_scores[x_key] = sorted(avg_scores[x_key])\n",
    "    \n",
    "    plt.plot(avg_scores[x_key], avg_scores['score'], color='red', label='large models')\n",
    "    plt.xlabel(x_key)\n",
    "    plt.ylabel('score')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if log_axis:\n",
    "        plt.xscale('log')\n",
    "\n",
    "    if title is not None:    \n",
    "        title_path = title.replace(' ', '_').replace(',', '_')\n",
    "        if not os.path.exists(wa.plot_path):\n",
    "            os.makedirs(wa.plot_path)\n",
    "        plt.savefig(os.path.join(wa.plot_path, f\"avg_scores_{title_path}.png\"))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "large_models = [\"pythia-12b\", \"pythia-6.9b\", \"pythia-2.8b\", \"pythia-1.4b\"]\n",
    "small_models = [\"pythia-160m\", \"pythia-70m\", \"pythia-31m\", \"pythia-14m\"]\n",
    "\n",
    "# avg_scores = plot_average_scores(dataset_scores, model_scores, color_mapping)\n",
    "avg_scores_large = plot_average_scores(dataset_scores, model_scores, \n",
    "                                      color_mapping, large_models,\n",
    "                                      log_axis=LOG_AXIS,\n",
    "                                      title=\"large_models, cov_mean, COMMON INSTANCES\",)\n",
    "avg_scores_small = plot_average_scores(dataset_scores, model_scores,\n",
    "                                        color_mapping, small_models,\n",
    "                                        log_axis=LOG_AXIS,\n",
    "                                        x_key='cov_mean',\n",
    "                                        title=\"small_models, cov_mean, COMMON INSTANCES\",)\n",
    "avg_scores = plot_average_scores(dataset_scores, model_scores,\n",
    "                                        color_mapping, models,\n",
    "                                        log_axis=LOG_AXIS,\n",
    "                                        x_key='cov_mean',\n",
    "                                        title=\"all_models, cov_mean, COMMON INSTANCES\",)\n",
    "\n",
    "# avg_scores_small = plot_average_scores(dataset_scores, model_scores, color_mapping, small_models)\n",
    "\n",
    "# plt.plot(avg_scores['n_samples'], avg_scores['score'], color='black', label='all models')\n",
    "# plt.plot(avg_scores_small['n_samples'], avg_scores_small['score'], color='blue', label='small models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large = plot_average_scores(dataset_scores, model_scores, \n",
    "                                       color_mapping, large_models,\n",
    "                                       log_axis=LOG_AXIS,\n",
    "                                       x_key='coverage',\n",
    "                                       title=f\"Large models, Coverage, NGRAM={N_GRAMS}\")\n",
    "\n",
    "avg_scores_small = plot_average_scores(dataset_scores, model_scores,\n",
    "                                       color_mapping, small_models,\n",
    "                                       log_axis=LOG_AXIS,\n",
    "                                       x_key='coverage',\n",
    "                                       title=f\"Small models, Coverage, NGRAM={N_GRAMS}\")\n",
    "\n",
    "avg_scores = plot_average_scores(dataset_scores, model_scores,\n",
    "                                 color_mapping, models,\n",
    "                                 log_axis=LOG_AXIS,\n",
    "                                 x_key='coverage',                                 title=f\"All models, Coverage, NGRAM={N_GRAMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Modified function to create an interactive plot using Plotly within a Jupyter Notebook\n",
    "def plot_average_scores_plotly(dataset_scores, model_scores, color_mapping, \n",
    "                              model_list=None, x_key='n_samples',\n",
    "                              log_axis=False, title=None):\n",
    "    avg_dataset_scores = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "    avg_scores = {'score': [], x_key: []} \n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for dataset in dataset_scores:\n",
    "        if model_list is None:\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean(dataset_scores[dataset]['score'])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean(dataset_scores[dataset][x_key])\n",
    "        else:\n",
    "            model_indices = [i for i, model in enumerate(model_scores.keys()) if model in model_list]\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean([dataset_scores[dataset]['score'][i] for i in model_indices])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean([dataset_scores[dataset][x_key][i] for i in model_indices])\n",
    "\n",
    "        # Append data for plotting\n",
    "        avg_scores['score'].append(avg_dataset_scores[dataset]['score'])\n",
    "        avg_scores[x_key].append(avg_dataset_scores[dataset][x_key])\n",
    "        \n",
    "        # Add trace for each dataset to the figure\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[avg_dataset_scores[dataset][x_key]],\n",
    "            y=[avg_dataset_scores[dataset]['score']],\n",
    "            text=[dataset],  # Will show this text on hover\n",
    "            mode='markers',\n",
    "            marker=dict(color=color_mapping[dataset]),\n",
    "            name=dataset\n",
    "        ))\n",
    "\n",
    "    # sort by x_key (e.g., 'n_samples')\n",
    "    sorted_indices = np.argsort(avg_scores[x_key])\n",
    "    sorted_scores = np.array(avg_scores['score'])[sorted_indices]\n",
    "    sorted_x_values = np.array(avg_scores[x_key])[sorted_indices]\n",
    "\n",
    "    # Add sorted average line to the figure\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sorted_x_values,\n",
    "        y=sorted_scores,\n",
    "        mode='lines',\n",
    "        marker=dict(color='red'),\n",
    "        name='Average'\n",
    "    ))\n",
    "\n",
    "    # Set log axis if specified\n",
    "    if log_axis:\n",
    "        fig.update_xaxes(type='log')\n",
    "\n",
    "    # Update layout with title and axis labels\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=x_key,\n",
    "        yaxis_title='Score',\n",
    "        hovermode='closest'\n",
    "    )\n",
    "\n",
    "    # disable legend\n",
    "    fig.update_layout(showlegend=False)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "avg_scores = plot_average_scores_plotly(dataset_scores, \n",
    "                                       model_scores, \n",
    "                                       color_mapping,\n",
    "                                       model_list=large_models,\n",
    "                                       log_axis=LOG_AXIS,\n",
    "                                       x_key='cov_mean',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dictionary is named `dataset_scores`\n",
    "sorted_data = sorted(dataset_scores.items(), key=lambda x: max(x[1]['n_samples']), reverse=True)\n",
    "top_5_datasets = {k: v['n_samples'] for k, v in sorted_data[:5]}\n",
    "print(f\"Top 5 datasets: {top_5_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "# wa.plot_scores(model_scores, dataset_scores, model_color_mapping, name=\"XY_pairs\")\n",
    "wa.plot_scores(model_scores, dataset_scores, color_mapping,\n",
    "               model_color_mapping, name=\"XY_pairs\",\n",
    "               log_axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa.plot_scores(single_model_score, single_model_dataset_score, color_mapping,\n",
    "#                model_color_mapping, name=\"XY_pairs\",\n",
    "#                log_axis=True, x_key='coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores, dataset_scores, color_mapping,\n",
    "            model_color_mapping, name=\"XY_pairs\",\n",
    "            x_key='coverage_', log_axis=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['pythia-12b', 'pythia-6.9b', 'pythia-2.8b', 'pythia-1.4b', 'pythia-410m', 'pythia-160m', 'pythia-70m', 'pythia-31m', 'pythia-14m']\n",
    "\n",
    "model_param_map = {'pythia-12b': 12e09,\n",
    "                    'pythia-6.9b': 6.9e09,\n",
    "                    'pythia-2.8b': 2.8e09,\n",
    "                    'pythia-1.4b': 1.4e09,\n",
    "                    'pythia-410m': 410e06,\n",
    "                    'pythia-160m': 160e06,\n",
    "                    'pythia-70m': 70e06,\n",
    "                    'pythia-31m': 31e06,\n",
    "                    'pythia-14m': 14e06,}\n",
    "\n",
    "# wa.plot_model_size_vs_scores(results_dict, models, model_param_map, color_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa.analyze_and_plot_distributions(task_dfs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total samples per language\n",
    "task_dfs_all_total_samples = {}\n",
    "for lang, df in task_dfs_all.items():\n",
    "    task_dfs_all_total_samples[lang] = df['value'].sum()\n",
    "\n",
    "model_scores_all, dataset_scores_all = wa.prepare_scores(results_dict, task_dfs_all, models,\n",
    "                                                         coverage_=task_cov_all_mean,\n",
    "                                                         cov_mean=task_cov_all_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gram_freqs_all['ja-en']['value'] = pd.to_numeric(n_gram_freqs_all['ja-en']['value'], errors='coerce')\n",
    "# largest_values = n_gram_freqs_all['ja-en'].nlargest(50, columns=['value'])\n",
    "\n",
    "# print(largest_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=f\"all instances NGRAM={N_GRAMS}\",\n",
    "            log_axis=LOG_AXIS)\n",
    "\n",
    "wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=f\"all instances NGRAM={N_GRAMS}\",\n",
    "            x_key=\"coverage\",\n",
    "            log_axis=LOG_AXIS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=f\"ALL INSTANCES NGRAM={N_GRAMS}\",\n",
    "            x_key=\"coverage_\",\n",
    "            log_axis=LOG_AXIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='coverage_', log_axis=False,\n",
    "                                          title=f\"Large models, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='coverage_', log_axis=False,\n",
    "                                          title=f\"Large models, Coverage_, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key='coverage_', log_axis=False,\n",
    "                                          title=f\"Small models, Coverage_, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, models,\n",
    "                                    x_key='coverage_', log_axis=False,\n",
    "                                    title=f\"All models, Coverage_, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='n_samples', log_axis=True,\n",
    "                                          title=f\"Large models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key='n_samples', log_axis=True,\n",
    "                                          title=f\"Small models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, models,\n",
    "                                    x_key='n_samples', log_axis=True,\n",
    "                                    title=f\"All models, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='coverage', log_axis=True,\n",
    "                                          title=f\"Large models, Coverage, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key='coverage', log_axis=True,\n",
    "                                          title=f\"Small models, Coverage, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, models,\n",
    "                                    x_key='coverage', log_axis=True,\n",
    "                                    title=f\"All models, Coverage, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_key = 'coverage'\n",
    "log_axis = True\n",
    "avg_scores_large_all = plot_average_scores_plotly(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key=plot_key, log_axis=log_axis,\n",
    "                                          title=f\"large models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores_plotly(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key=plot_key, log_axis=log_axis,\n",
    "                                          title=f\"small models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores_plotly(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, None,\n",
    "                                    x_key=plot_key, log_axis=log_axis,\n",
    "                                    title=f\"all models, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa.plot_scores(model_scores, dataset_scores, \n",
    "#             color_mapping, model_color_mapping, \n",
    "#             name=\"XY_pairs\", log_axis=LOG_AXIS,\n",
    "#             x_key=\"coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in task_dfs_all['world_religions']['example']:\n",
    "#     print(example['question'])\n",
    "#     break\n",
    "\n",
    "# examples = list(task_dfs_all['world_religions']['example'])\n",
    "# examples[5]['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the contents of results.json for each model/dataset pair\n",
    "results_dict = collections.defaultdict(dict)\n",
    "instance_results_dict = collections.defaultdict(dict)\n",
    "\n",
    "# Iterate over each model and dataset, loading the results.json file\n",
    "\n",
    "for base_results_path, models_ in base_results_paths.items():\n",
    "    for model in models_:\n",
    "        results_path = os.path.join(base_results_path, model, TASK, SHOT, 'results.json')\n",
    "        instance_results_path = os.path.join(base_results_path, model, TASK, SHOT, 'doc_results.json')\n",
    "        results = json.load(open(results_path, 'r'))['results']\n",
    "        doc_results = json.load(open(instance_results_path, 'r'))\n",
    "        for task in results.keys():\n",
    "            # remove hendrycksTest-\n",
    "            task_str = task.split(\"-\")[1]\n",
    "            if task_str in TASKS:\n",
    "                results_dict[task_str][model] = results[task]['acc']\n",
    "                if os.path.exists(instance_results_path):\n",
    "                    instance_results_dict[model][task_str] = doc_results[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance_results_dict['pythia-12b']['world_religions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_instance_results(instance_results, task_dfs, label=''):\n",
    "    model_instance_results = collections.defaultdict(list)\n",
    "    for model in instance_results.keys():\n",
    "        for i, task in enumerate(instance_results[model].keys()):\n",
    "            model_task_results = instance_results[model][task].copy()\n",
    "            # Add task label to each id, with an optional common label\n",
    "            for row in model_task_results:\n",
    "                row['id'] = f\"{str(row['id'])}_{i}\"\n",
    "            model_instance_results[model].extend(model_task_results)\n",
    "    return model_instance_results\n",
    "\n",
    "def merge_and_process_dfs(task_dfs):\n",
    "    example_dfs = pd.concat(task_dfs.values())\n",
    "    # Extract 'query' from the 'example' column\n",
    "    example_dfs['query'] = example_dfs['example'].apply(lambda x: x['question'])\n",
    "    # Perform the aggregation to get the sum of 'value' and other statistics if needed\n",
    "    aggregated_data = example_dfs.groupby(['query', 'task'])['value'].agg(['sum', 'count']).reset_index()\n",
    "    # Merge the aggregated data back to the original DataFrame\n",
    "    example_dfs = example_dfs.merge(aggregated_data, on=['query', 'task'], how='left')\n",
    "    # Sort the DataFrame based on the 'sum' of 'value'\n",
    "    example_dfs = example_dfs.sort_values(by='sum', ascending=False)\n",
    "    # rename \"pair\" column to query\n",
    "    # example_dfs = example_dfs.rename(columns={'pair': 'query'})\n",
    "    \n",
    "    # example_dfs = example_dfs.groupby(['query', 'task'])['value'].sum().sort_values(ascending=False).reset_index()\n",
    "    return example_dfs\n",
    "\n",
    "# Process all tasks\n",
    "model_instance_results_all = process_instance_results(instance_results_dict.copy(), task_dfs_all.copy(), label='_all')\n",
    "example_dfs_all = merge_and_process_dfs(task_dfs_all)\n",
    "\n",
    "# Process common tasks\n",
    "model_instance_results_common = process_instance_results(instance_results_dict.copy(), task_dfs_common.copy(), label='_common')\n",
    "example_dfs_common = merge_and_process_dfs(task_dfs_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rapidfuzz import process, fuzz\n",
    "\n",
    "# # Create a new column for the id\n",
    "# example_dfs_all['id'] = None\n",
    "# instances = pd.DataFrame(model_instance_results['pythia-12b'])\n",
    "\n",
    "# for instance in tqdm(instances):\n",
    "#     # Skip if this instance has already been matched\n",
    "#     # if example_dfs_all.loc[example_dfs_all['query'] == instance['query'], 'id'].notna().any():\n",
    "#         # continue\n",
    "\n",
    "#     # Get the best match in example_dfs_all['query']\n",
    "#     best_match, score, _ = process.extractOne(instance['query'], example_dfs_all['query'], scorer=fuzz.token_sort_ratio)\n",
    "    \n",
    "#     # If the score is above a certain threshold, assign the id\n",
    "#     example_dfs_all.loc[example_dfs_all['query'] == best_match, 'id'] = instance['id']\n",
    "\n",
    "# # Save the DataFrame to a CSV file in BASE_PATH\n",
    "# example_dfs_all.to_csv(os.path.join(BASE_PATH, 'example_dfs_all_.csv'), index=False)\n",
    "\n",
    "# # instances = model_instance_results['pythia-12b']\n",
    "# # for row, example in enumerate(tqdm(example_dfs_all['query'])):\n",
    "# #     for instance in instances:\n",
    "# #         if example == instance['query']:\n",
    "# #             print(f\"Found match for {example}\")\n",
    "# #             example_dfs_all.loc[row, 'id'] = instance['id']\n",
    "# #             instances.remove(instance)\n",
    "# #             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def normalize_text(text):\n",
    "#     # Convert to lowercase, strip whitespace, remove punctuation, etc.\n",
    "#     return text.lower().strip()\n",
    "\n",
    "# def match_and_update_ids(example_dfs, model_instance_results, filename, key='query'):\n",
    "#     # Convert model_instance_results to a DataFrame\n",
    "#     instances = pd.DataFrame(model_instance_results)\n",
    "\n",
    "#     # Normalize the 'query' column in both DataFrames\n",
    "#     example_dfs[key] = example_dfs[key].apply(normalize_text)\n",
    "#     instances[key] = instances[key].apply(normalize_text)\n",
    "\n",
    "#     print(f\"example_dfs: {example_dfs.head(1)}\")\n",
    "#     print(f\"instances: {instances.head(1)}\")\n",
    "#     # Merge the example_dfs with instances on 'query' column\n",
    "#     merged_df = example_dfs.merge(instances[['id', key]], on=key, how='left')\n",
    "\n",
    "#     # Save the merged DataFrame to a CSV file if it doesn't exist\n",
    "#     if not os.path.exists(filename):\n",
    "#         merged_df.to_csv(filename, index=False)\n",
    "    \n",
    "#     return merged_df\n",
    "\n",
    "# # Assuming BASE_PATH is defined\n",
    "# # Process all tasks\n",
    "# example_dfs_all_filename = os.path.join(BASE_PATH, 'example_dfs_all_exact.csv')\n",
    "# example_dfs_all = match_and_update_ids(example_dfs_all, model_instance_results_all['pythia-12b'], example_dfs_all_filename)\n",
    "\n",
    "# # Process common tasks\n",
    "# example_dfs_common_filename = os.path.join(BASE_PATH, 'example_dfs_common_exact.csv')\n",
    "# example_dfs_common = match_and_update_ids(example_dfs_common, model_instance_results_common['pythia-12b'], example_dfs_common_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4973/450981 [00:09<13:43, 541.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m example_dfs_common_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_dfs_common_exact.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Process all tasks\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# example_dfs_all = add_ids_and_save(example_dfs_all, instances_all_df, example_dfs_all_filename)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Process common tasks\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m example_dfs_common \u001b[38;5;241m=\u001b[39m \u001b[43madd_ids_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_dfs_common\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstances_common_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_dfs_common_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[90], line 13\u001b[0m, in \u001b[0;36madd_ids_and_save\u001b[0;34m(example_dfs, instances, filename)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_ids_and_save\u001b[39m(example_dfs, instances, filename):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Add a new column 'id' to example_dfs by applying the find_matching_id function with a progress bar\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     example_dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mexample_dfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_matching_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstances_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Save the DataFrame to CSV\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     example_dfs\u001b[38;5;241m.\u001b[39mto_csv(filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/tqdm/std.py:805\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/apply.py:133\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/tqdm/std.py:800\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    799\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[90], line 6\u001b[0m, in \u001b[0;36mfind_matching_id\u001b[0;34m(row, instances_df)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_matching_id\u001b[39m(row, instances_df):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Use regex=False to avoid interpreting the pattern as a regular expression\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     match \u001b[38;5;241m=\u001b[39m instances_df[\u001b[43minstances_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m match\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/strings/accessor.py:129\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/strings/accessor.py:1263\u001b[0m, in \u001b[0;36mStringMethods.contains\u001b[0;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regex \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39mcompile(pat)\u001b[38;5;241m.\u001b[39mgroups:\n\u001b[1;32m   1256\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis pattern is interpreted as a regular expression, and has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch groups. To actually get the groups, use str.extract.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1259\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1260\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1261\u001b[0m     )\n\u001b[0;32m-> 1263\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_contains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_result(result, fill_value\u001b[38;5;241m=\u001b[39mna, returns_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/strings/object_array.py:143\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_contains\u001b[0;34m(self, pat, case, flags, na, regex)\u001b[0m\n\u001b[1;32m    141\u001b[0m         upper_pat \u001b[38;5;241m=\u001b[39m pat\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m    142\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: upper_pat \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/strings/object_array.py:76\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[0;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[1;32m     74\u001b[0m map_convert \u001b[38;5;241m=\u001b[39m convert \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_convert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     p_err \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m((takes)|(missing)) (?(2)from \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ to )?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?(3)required )positional arguments?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "def find_matching_id(row, instances_df):\n",
    "    # Use regex=False to avoid interpreting the pattern as a regular expression\n",
    "    match = instances_df[instances_df['query'].str.contains(row['query'], na=False, regex=False)]\n",
    "    if not match.empty:\n",
    "        return match.iloc[0]['id']\n",
    "    return None\n",
    "\n",
    "def add_ids_and_save(example_dfs, instances, filename):\n",
    "    # Add a new column 'id' to example_dfs by applying the find_matching_id function with a progress bar\n",
    "    example_dfs['id'] = example_dfs.progress_apply(find_matching_id, axis=1, instances_df=instances)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    example_dfs.to_csv(filename, index=False)\n",
    "\n",
    "    return example_dfs\n",
    "\n",
    "# Assuming model_instance_results is a list of dictionaries\n",
    "instances_all_df = pd.DataFrame(model_instance_results_all['pythia-12b'])\n",
    "instances_common_df = pd.DataFrame(model_instance_results_common['pythia-12b'])\n",
    "\n",
    "# Assuming BASE_PATH is defined\n",
    "example_dfs_all_filename = os.path.join(BASE_PATH, 'example_dfs_all_exact.csv')\n",
    "example_dfs_common_filename = os.path.join(BASE_PATH, 'example_dfs_common_exact.csv')\n",
    "\n",
    "# Process all tasks\n",
    "# example_dfs_all = add_ids_and_save(example_dfs_all, instances_all_df, example_dfs_all_filename)\n",
    "\n",
    "# Process common tasks\n",
    "example_dfs_common = add_ids_and_save(example_dfs_common, instances_common_df, example_dfs_common_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it\n",
    "# example_dfs_all_0 = pd.read_csv(os.path.join(BASE_PATH, \"_0\", \"example_dfs_all_exact_0.csv\"))\n",
    "# example_dfs_common_0 = pd.read_csv(os.path.join(BASE_PATH, \"_0\", \"example_dfs_common_exact_0.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load it\n",
    "# examples_dfs_all = pd.read_csv(os.path.join(BASE_PATH, 'example_dfs_all_exact.csv'))\n",
    "# examples_dfs_common = pd.read_csv(os.path.join(BASE_PATH, 'example_dfs_common_exact.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # subtract max(x) for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def nll_to_prob(nll):\n",
    "    return np.exp(nll)\n",
    "\n",
    "def process_and_save_results(example_dfs, model_instance_results, base_path, suffix):\n",
    "    examples_dfs_models = {}\n",
    "    for model in model_instance_results.keys():\n",
    "        instances = pd.DataFrame(model_instance_results[model])\n",
    "        examples_dfs_model = example_dfs.merge(instances, on='id', how='inner')\n",
    "        examples_dfs_model['probs_softmax'] = examples_dfs_model['lls'].apply(softmax)\n",
    "        examples_dfs_model['probs_nll'] = examples_dfs_model['lls'].apply(nll_to_prob)\n",
    "        examples_dfs_model['probs_softmax_gold'] = examples_dfs_model.apply(lambda row: row['probs_softmax'][row['gold']], axis=1)\n",
    "        examples_dfs_model['probs_nll_gold'] = examples_dfs_model.apply(lambda row: row['probs_nll'][row['gold']], axis=1)\n",
    "        examples_dfs_models[model] = examples_dfs_model\n",
    "\n",
    "    # Save model_instance_results to pickle\n",
    "    with open(os.path.join(base_path, f'model_instance_results_{suffix}.pkl'), 'wb') as f:\n",
    "        pickle.dump(model_instance_results, f)\n",
    "\n",
    "    # Save examples_dfs_models to pickle\n",
    "    with open(os.path.join(base_path, f'examples_dfs_{suffix}_models.pkl'), 'wb') as f:\n",
    "        pickle.dump(examples_dfs_models, f)\n",
    "    \n",
    "    # # save example_dfs to pkl\n",
    "    # with open(os.path.join(base_path, f'example_dfs_{suffix}.pkl'), 'wb') as f:\n",
    "    #     pickle.dump(example_dfs, f)\n",
    "\n",
    "    return examples_dfs_models\n",
    "    \n",
    "    \n",
    "\n",
    "# Assuming BASE_PATH is defined\n",
    "# Process all tasks\n",
    "# example_dfs_all_models = process_and_save_results(example_dfs_all, model_instance_results_all, BASE_PATH, 'all')\n",
    "\n",
    "# Process common tasks\n",
    "example_dfs_common_models = process_and_save_results(example_dfs_common, model_instance_results_common, BASE_PATH, 'common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many examples are not None or Nan\n",
    "examples_matched = example_dfs_all['id'].notna().sum()\n",
    "examples_umatched = example_dfs_all['id'].isna().sum()\n",
    "\n",
    "print(f\"Examples matched: {examples_matched}\")\n",
    "print(f\"Examples unmatched: {examples_umatched}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
