{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "import langid\n",
    "from src.wimbd_ import WimbdAnalysis, load_mmlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_freqs_path = \"./results/n-grams/exp_full/2/common/pl-en-2-grams.pkl\"\n",
    "n_gram_freqs = pickle.load(open(n_gram_freqs_path, \"rb\"))\n",
    "n_gram_freqs = dict(sorted(n_gram_freqs.items(), key=lambda item: item[1]['value'], reverse=True))\n",
    "# # export as text\n",
    "# # with open(\"n_gram_freqs.txt\", \"w\") as f:\n",
    "# #     for k, v in n_gr\n",
    "# # 3am_freqs.items():\n",
    "# #         f.write(f\"{k}: {v}\\n\")\n",
    "# LANGUAGES = ['ru-en', 'fr-en', 'ro-en', 'de-en', 'pl-en', 'cs-en']\n",
    "# LANGUAGES = ['ru-en', 'ro-en', 'de-en', 'pl-en', 'cs-en', 'fr-en', 'ja-en', 'zh-en']\n",
    "\n",
    "# wmt09\n",
    "ds = load_mmlu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang params\n",
    "N_GRAMS = 5\n",
    "# BASE_DIR = \"./results/n-grams/mmlu/test-set/exp_full_None\"\n",
    "# BASE_DIR = \"./results/n-grams/mmlu/exp3/test-set/exp_full_None\"\n",
    "# BASE_DIR = \"./results/n-grams/mmlu/pile/exp4_filter/test-set/exp_full_None\"\n",
    "BASE_DIR = \"./results/n-grams/mmlu/pile/exp4_nofilter/test-set/exp_full_None\"\n",
    "BASE_PATH = os.path.join(BASE_DIR, f\"{N_GRAMS}\")\n",
    "BASE_PATH_COMMON = os.path.join(BASE_PATH, \"common\")\n",
    "BASE_PATH_ALL = os.path.join(BASE_PATH, \"all\")\n",
    "FILTER_CHARS = False\n",
    "DETECT_LANG = False\n",
    "LOG_AXIS = True\n",
    "TASK = \"MMLU/hendrycks*\"\n",
    "SHOT = \"0-shot\"\n",
    "\n",
    "# model params\n",
    "base_results_paths = {\n",
    "    \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/pythia/experiment_4/inference/EleutherAI\": [\n",
    "        'pythia-12b', 'pythia-6.9b', 'pythia-2.8b', \n",
    "        'pythia-1.4b', 'pythia-410m', 'pythia-160m', \n",
    "        'pythia-70m', 'pythia-31m', 'pythia-14m'\n",
    "    ],\n",
    "    \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/OLMO\": [\n",
    "        'OLMo-7b'\n",
    "    ]\n",
    "}\n",
    "\n",
    "models = ['pythia-12b', 'pythia-6.9b', 'pythia-2.8b', 'pythia-1.4b', 'pythia-410m', 'pythia-160m', 'pythia-70m', 'pythia-31m', 'pythia-14m',\n",
    "          'OLMo-7b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS_OMMIT = [\"formal_logic\"]\n",
    "TASKS = [t for t in list(ds.keys()) if t not in TASKS_OMMIT]\n",
    "wa = WimbdAnalysis(BASE_PATH, TASKS, N_GRAMS, FILTER_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs = wa.get_task_dfs(BASE_PATH_COMMON, TASKS, filter_chars=FILTER_CHARS)\n",
    "task_dfs_filtered = wa.get_task_dfs(BASE_PATH, TASKS, filter_chars=True)\n",
    "# task_dfs\n",
    "\n",
    "# get total samples per language pair\n",
    "task_dfs_total_samples = {lang: df['value'].sum() for lang, df in task_dfs.items()}\n",
    "colors = sns.color_palette('hls', len(task_dfs))\n",
    "color_mapping = {lang: color for lang, color in zip(task_dfs.keys(), colors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = {task: ds[task] for task in task_dfs.keys()}\n",
    "# TASKS = [t for t in list(ds.keys()) if t not in TASKS_OMMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "for task in TASKS:\n",
    "    total_samples += task_dfs_total_samples[task]\n",
    "print(f\"Total samples: {total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_path_common = os.path.join(BASE_PATH_COMMON, \"task-coverage.pkl\")\n",
    "coverage_path_all = os.path.join(BASE_PATH_ALL, \"task-coverage.pkl\")\n",
    "\n",
    "with open(coverage_path_common, \"rb\") as f:\n",
    "    coverage_common = pickle.load(f)\n",
    "\n",
    "with open(coverage_path_all, \"rb\") as f:\n",
    "    coverage_all = pickle.load(f)\n",
    "\n",
    "coverage_common = pd.DataFrame(coverage_common)\n",
    "coverage_common = coverage_common[coverage_common['task'].isin(TASKS)]\n",
    "\n",
    "coverage_all = pd.DataFrame(coverage_all)\n",
    "coverage_all = coverage_all[coverage_all['task'].isin(TASKS)]\n",
    "\n",
    "task_cov_common = coverage_common.groupby('task')['coverage'].mean().to_dict()\n",
    "task_cov_all = coverage_all.groupby('task')['coverage'].mean().to_dict()\n",
    "\n",
    "\n",
    "# now get avg task coverage\n",
    "N_GRAM_LIST = N_GRAMS if isinstance(N_GRAMS, list) else [N_GRAMS]\n",
    "task_coverage = defaultdict(list)\n",
    "for n_grams in N_GRAM_LIST:\n",
    "    coverage_path_common = os.path.join(BASE_PATH_COMMON, \"task-coverage.pkl\")\n",
    "    coverage_path_all = os.path.join(BASE_PATH_ALL, \"task-coverage.pkl\")\n",
    "\n",
    "    with open(coverage_path_common, \"rb\") as f:\n",
    "        coverage_common = pickle.load(f)\n",
    "\n",
    "    with open(coverage_path_all, \"rb\") as f:\n",
    "        coverage_all = pickle.load(f)\n",
    "\n",
    "    coverage_common = pd.DataFrame(coverage_common)\n",
    "    coverage_common = coverage_common[coverage_common['task'].isin(TASKS)]\n",
    "\n",
    "    coverage_all = pd.DataFrame(coverage_all)\n",
    "    coverage_all = coverage_all[coverage_all['task'].isin(TASKS)]\n",
    "\n",
    "    task_cov_common = coverage_common.groupby('task')['coverage'].mean().to_dict()\n",
    "    task_cov_all = coverage_all.groupby('task')['coverage'].mean().to_dict()\n",
    "\n",
    "    for task in TASKS:\n",
    "        task_coverage[task].append(task_cov_common[task])\n",
    "\n",
    "task_coverage = pd.DataFrame(task_coverage)\n",
    "task_coverage = task_coverage.melt().rename(columns={\"variable\": \"task\", \"value\": \"coverage\"})\n",
    "task_cov_mean = task_coverage.groupby('task')['coverage'].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_results_pth = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/scrap/doc_results.json\"\n",
    "with open(doc_results_pth, \"r\") as f:\n",
    "    doc_results = json.load(f)\n",
    "\n",
    "doc_results[0]['query']\n",
    "\n",
    "print(len(doc_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs_all = wa.get_task_dfs(BASE_PATH_ALL, TASKS)\n",
    "task_dfs_common = wa.get_task_dfs(BASE_PATH_COMMON, TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs_common.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export as .pkl\n",
    "# # save all tasks .pkl\n",
    "\n",
    "# with open(os.path.join(BASE_PATH, \"task_dfs_all.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(task_dfs_all, f)\n",
    "\n",
    "# with open(os.path.join(BASE_PATH, \"task_dfs_common.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(task_dfs_common, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs_common['college_mathematics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dfs_common['college_mathematics']['example'].iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/results/n-grams/mmlu/exp1/4/common/task-coverage.pkl\"\n",
    "\n",
    "with open(pth, \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f\"./results/n-grams/exp_full/{N_GRAMS}/common\"\n",
    "# # for each language, save top 100 examples in a csv\n",
    "\n",
    "# for lang, df in lang_dfs.items():\n",
    "#     df = df.sort_values(by=['value'], ascending=False)\n",
    "#     df.to_csv(os.path.join(save_path, f\"{lang}_samples.csv\"), index=False)\n",
    "\n",
    "# for lang, df in lang_dfs_filtered.items():\n",
    "#     df = df.sort_values(by=['value'], ascending=False)\n",
    "#     df = df.head(100)\n",
    "#     df.to_csv(os.path.join(save_path, f\"{lang}_samples_filtered.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gram_freqs.pop(('angela merkel', 'chancellor angela'))\n",
    "n_gram_freqs_values = [v['value'] for v in n_gram_freqs.values()]\n",
    "n_gram_freqs_values_cumsum = np.cumsum(n_gram_freqs_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage:\n",
    "# wa.plot_n_samples(task_dfs, color_mapping)\n",
    "# # wa.plot_n_samples(lang_dfs_filtered, color_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_colormap = plt.cm.get_cmap('coolwarm', len(models))\n",
    "model_color_mapping = {model: model_colormap(1 - i / len(models)) for i, model in enumerate(models)}\n",
    "\n",
    "\n",
    "# wa.analyze_and_plot_distributions(task_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the contents of results.json for each model/dataset pair\n",
    "results_dict = collections.defaultdict(dict)\n",
    "instance_results_dict = collections.defaultdict(dict)\n",
    "\n",
    "# Iterate over each model and dataset, loading the results.json file\n",
    "\n",
    "for base_results_path, models_ in base_results_paths.items():\n",
    "    for model in models_:\n",
    "        results_path = os.path.join(base_results_path, model, TASK, SHOT, 'results.json')\n",
    "        instance_results_path = os.path.join(base_results_path, model, TASK, SHOT, 'doc_results.json')\n",
    "        results = json.load(open(results_path, 'r'))['results']\n",
    "        doc_results = json.load(open(instance_results_path, 'r'))\n",
    "        for task in results.keys():\n",
    "            # remove hendrycksTest-\n",
    "            task_str = task.split(\"-\")[1]\n",
    "            if task_str in TASKS:\n",
    "                results_dict[task_str][model] = results[task]['acc']\n",
    "                if os.path.exists(instance_results_path):\n",
    "                    instance_results_dict[model][task_str] = doc_results[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_bleu_scores(results_dict, models, color_mapping)\n",
    "\n",
    "task_ds = ds\n",
    "\n",
    "# task_mutual_info, merged_mutual_info = wa.calculate_mutual_info(task_dfs, task_ds)\n",
    "task_mutual_info, merged_mutual_info = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores, dataset_scores = wa.prepare_scores(results_dict, task_dfs, \n",
    "                                                 models, coverage_=task_cov_common,\n",
    "                                                 cov_mean=task_cov_mean)\n",
    "\n",
    "single_model = [\"pythia-12b\"]\n",
    "single_model_score, single_model_dataset_score = wa.prepare_scores(results_dict, task_dfs, \n",
    "                                                                   single_model, coverage_=task_cov_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_scores(dataset_scores, model_scores, color_mapping, \n",
    "                       model_list=None, x_key='n_samples',\n",
    "                       log_axis=False, title=None, annotate=False):\n",
    "    avg_dataset_scores = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "    avg_scores = {'score': [], x_key: []} \n",
    "\n",
    "    for dataset in dataset_scores:\n",
    "        if model_list is None:\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean(dataset_scores[dataset]['score'])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean(dataset_scores[dataset][x_key])\n",
    "        else:\n",
    "            model_indices = [i for i, model in enumerate(model_scores.keys()) if model in model_list]\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean([dataset_scores[dataset]['score'][i] for i in model_indices])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean([dataset_scores[dataset][x_key][i] for i in model_indices])\n",
    "\n",
    "        plt.scatter(avg_dataset_scores[dataset][x_key], avg_dataset_scores[dataset]['score'], \n",
    "                    color=color_mapping[dataset], label=dataset)\n",
    "        if annotate:\n",
    "            plt.annotate(dataset, (avg_dataset_scores[dataset][x_key], avg_dataset_scores[dataset]['score']))\n",
    "        avg_scores['score'].append(avg_dataset_scores[dataset]['score'])\n",
    "        avg_scores[x_key].append(avg_dataset_scores[dataset][x_key])\n",
    "\n",
    "    # sort by n_samples\n",
    "    avg_scores['score'] = [x for _, x in sorted(zip(avg_scores[x_key], avg_scores['score']))]\n",
    "    avg_scores[x_key] = sorted(avg_scores[x_key])\n",
    "    \n",
    "    plt.plot(avg_scores[x_key], avg_scores['score'], color='red', label='large models')\n",
    "    plt.xlabel(x_key)\n",
    "    plt.ylabel('score')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if log_axis:\n",
    "        plt.xscale('log')\n",
    "\n",
    "    if title is not None:    \n",
    "        title_path = title.replace(' ', '_').replace(',', '_')\n",
    "        if not os.path.exists(wa.plot_path):\n",
    "            os.makedirs(wa.plot_path)\n",
    "        plt.savefig(os.path.join(wa.plot_path, f\"avg_scores_{title_path}.png\"))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "large_models = [\"pythia-12b\", \"pythia-6.9b\", \"pythia-2.8b\", \"pythia-1.4b\"]\n",
    "small_models = [\"pythia-160m\", \"pythia-70m\", \"pythia-31m\", \"pythia-14m\"]\n",
    "\n",
    "# avg_scores = plot_average_scores(dataset_scores, model_scores, color_mapping)\n",
    "avg_scores_large = plot_average_scores(dataset_scores, model_scores, \n",
    "                                      color_mapping, large_models,\n",
    "                                      log_axis=LOG_AXIS,\n",
    "                                      title=\"large_models, cov_mean, COMMON INSTANCES\",)\n",
    "avg_scores_small = plot_average_scores(dataset_scores, model_scores,\n",
    "                                        color_mapping, small_models,\n",
    "                                        log_axis=LOG_AXIS,\n",
    "                                        x_key='cov_mean',\n",
    "                                        title=\"small_models, cov_mean, COMMON INSTANCES\",)\n",
    "avg_scores = plot_average_scores(dataset_scores, model_scores,\n",
    "                                        color_mapping, models,\n",
    "                                        log_axis=LOG_AXIS,\n",
    "                                        x_key='cov_mean',\n",
    "                                        title=\"all_models, cov_mean, COMMON INSTANCES\",)\n",
    "\n",
    "# avg_scores_small = plot_average_scores(dataset_scores, model_scores, color_mapping, small_models)\n",
    "\n",
    "# plt.plot(avg_scores['n_samples'], avg_scores['score'], color='black', label='all models')\n",
    "# plt.plot(avg_scores_small['n_samples'], avg_scores_small['score'], color='blue', label='small models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large = plot_average_scores(dataset_scores, model_scores, \n",
    "                                       color_mapping, large_models,\n",
    "                                       log_axis=LOG_AXIS,\n",
    "                                       x_key='coverage',\n",
    "                                       title=f\"Large models, Coverage, NGRAM={N_GRAMS}\")\n",
    "\n",
    "avg_scores_small = plot_average_scores(dataset_scores, model_scores,\n",
    "                                       color_mapping, small_models,\n",
    "                                       log_axis=LOG_AXIS,\n",
    "                                       x_key='coverage',\n",
    "                                       title=f\"Small models, Coverage, NGRAM={N_GRAMS}\")\n",
    "\n",
    "avg_scores = plot_average_scores(dataset_scores, model_scores,\n",
    "                                 color_mapping, models,\n",
    "                                 log_axis=LOG_AXIS,\n",
    "                                 x_key='coverage',                                 title=f\"All models, Coverage, NGRAM={N_GRAMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Modified function to create an interactive plot using Plotly within a Jupyter Notebook\n",
    "def plot_average_scores_plotly(dataset_scores, model_scores, color_mapping, \n",
    "                              model_list=None, x_key='n_samples',\n",
    "                              log_axis=False, title=None):\n",
    "    avg_dataset_scores = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "    avg_scores = {'score': [], x_key: []} \n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for dataset in dataset_scores:\n",
    "        if model_list is None:\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean(dataset_scores[dataset]['score'])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean(dataset_scores[dataset][x_key])\n",
    "        else:\n",
    "            model_indices = [i for i, model in enumerate(model_scores.keys()) if model in model_list]\n",
    "            avg_dataset_scores[dataset]['score'] = np.mean([dataset_scores[dataset]['score'][i] for i in model_indices])\n",
    "            avg_dataset_scores[dataset][x_key] = np.mean([dataset_scores[dataset][x_key][i] for i in model_indices])\n",
    "\n",
    "        # Append data for plotting\n",
    "        avg_scores['score'].append(avg_dataset_scores[dataset]['score'])\n",
    "        avg_scores[x_key].append(avg_dataset_scores[dataset][x_key])\n",
    "        \n",
    "        # Add trace for each dataset to the figure\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[avg_dataset_scores[dataset][x_key]],\n",
    "            y=[avg_dataset_scores[dataset]['score']],\n",
    "            text=[dataset],  # Will show this text on hover\n",
    "            mode='markers',\n",
    "            marker=dict(color=color_mapping[dataset]),\n",
    "            name=dataset\n",
    "        ))\n",
    "\n",
    "    # sort by x_key (e.g., 'n_samples')\n",
    "    sorted_indices = np.argsort(avg_scores[x_key])\n",
    "    sorted_scores = np.array(avg_scores['score'])[sorted_indices]\n",
    "    sorted_x_values = np.array(avg_scores[x_key])[sorted_indices]\n",
    "\n",
    "    # Add sorted average line to the figure\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sorted_x_values,\n",
    "        y=sorted_scores,\n",
    "        mode='lines',\n",
    "        marker=dict(color='red'),\n",
    "        name='Average'\n",
    "    ))\n",
    "\n",
    "    # Set log axis if specified\n",
    "    if log_axis:\n",
    "        fig.update_xaxes(type='log')\n",
    "\n",
    "    # Update layout with title and axis labels\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=x_key,\n",
    "        yaxis_title='Score',\n",
    "        hovermode='closest'\n",
    "    )\n",
    "\n",
    "    # disable legend\n",
    "    fig.update_layout(showlegend=False)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "avg_scores = plot_average_scores_plotly(dataset_scores, \n",
    "                                       model_scores, \n",
    "                                       color_mapping,\n",
    "                                       model_list=large_models,\n",
    "                                       log_axis=LOG_AXIS,\n",
    "                                       x_key='cov_mean',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dictionary is named `dataset_scores`\n",
    "sorted_data = sorted(dataset_scores.items(), key=lambda x: max(x[1]['n_samples']), reverse=True)\n",
    "top_5_datasets = {k: v['n_samples'] for k, v in sorted_data[:5]}\n",
    "print(f\"Top 5 datasets: {top_5_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "# wa.plot_scores(model_scores, dataset_scores, model_color_mapping, name=\"XY_pairs\")\n",
    "wa.plot_scores(model_scores, dataset_scores, color_mapping,\n",
    "               model_color_mapping, name=\"XY_pairs\",\n",
    "               log_axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa.plot_scores(single_model_score, single_model_dataset_score, color_mapping,\n",
    "#                model_color_mapping, name=\"XY_pairs\",\n",
    "#                log_axis=True, x_key='coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores, dataset_scores, color_mapping,\n",
    "            model_color_mapping, name=\"XY_pairs\",\n",
    "            x_key='coverage_', log_axis=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['pythia-12b', 'pythia-6.9b', 'pythia-2.8b', 'pythia-1.4b', 'pythia-410m', 'pythia-160m', 'pythia-70m', 'pythia-31m', 'pythia-14m']\n",
    "\n",
    "model_param_map = {'pythia-12b': 12e09,\n",
    "                    'pythia-6.9b': 6.9e09,\n",
    "                    'pythia-2.8b': 2.8e09,\n",
    "                    'pythia-1.4b': 1.4e09,\n",
    "                    'pythia-410m': 410e06,\n",
    "                    'pythia-160m': 160e06,\n",
    "                    'pythia-70m': 70e06,\n",
    "                    'pythia-31m': 31e06,\n",
    "                    'pythia-14m': 14e06,}\n",
    "\n",
    "# wa.plot_model_size_vs_scores(results_dict, models, model_param_map, color_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa.analyze_and_plot_distributions(task_dfs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total samples per language\n",
    "task_dfs_all_total_samples = {}\n",
    "for lang, df in task_dfs_all.items():\n",
    "    task_dfs_all_total_samples[lang] = df['value'].sum()\n",
    "\n",
    "model_scores_all, dataset_scores_all = wa.prepare_scores(results_dict, task_dfs_all, models,\n",
    "                                                         coverage_=task_cov_all,\n",
    "                                                         cov_mean=task_cov_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gram_freqs_all['ja-en']['value'] = pd.to_numeric(n_gram_freqs_all['ja-en']['value'], errors='coerce')\n",
    "# largest_values = n_gram_freqs_all['ja-en'].nlargest(50, columns=['value'])\n",
    "\n",
    "# print(largest_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=f\"all instances NGRAM={N_GRAMS}\",\n",
    "            log_axis=LOG_AXIS)\n",
    "\n",
    "wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=f\"all instances NGRAM={N_GRAMS}\",\n",
    "            x_key=\"coverage\",\n",
    "            log_axis=LOG_AXIS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa.plot_scores(model_scores_all, dataset_scores_all, \n",
    "            color_mapping, model_color_mapping, \n",
    "            name=f\"ALL INSTANCES NGRAM={N_GRAMS}\",\n",
    "            x_key=\"coverage_\",\n",
    "            log_axis=LOG_AXIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='coverage_', log_axis=False,\n",
    "                                          title=f\"Large models, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='coverage_', log_axis=False,\n",
    "                                          title=f\"Large models, Coverage_, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key='coverage_', log_axis=False,\n",
    "                                          title=f\"Small models, Coverage_, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, models,\n",
    "                                    x_key='coverage_', log_axis=False,\n",
    "                                    title=f\"All models, Coverage_, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='n_samples', log_axis=True,\n",
    "                                          title=f\"Large models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key='n_samples', log_axis=True,\n",
    "                                          title=f\"Small models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, models,\n",
    "                                    x_key='n_samples', log_axis=True,\n",
    "                                    title=f\"All models, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores_large_all = plot_average_scores(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key='coverage', log_axis=True,\n",
    "                                          title=f\"Large models, Coverage, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key='coverage', log_axis=True,\n",
    "                                          title=f\"Small models, Coverage, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, models,\n",
    "                                    x_key='coverage', log_axis=True,\n",
    "                                    title=f\"All models, Coverage, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_key = 'coverage'\n",
    "log_axis = True\n",
    "avg_scores_large_all = plot_average_scores_plotly(dataset_scores_all, model_scores_all, \n",
    "                                          color_mapping, large_models,\n",
    "                                          x_key=plot_key, log_axis=log_axis,\n",
    "                                          title=f\"large models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_small_all = plot_average_scores_plotly(dataset_scores_all, model_scores_all,\n",
    "                                          color_mapping, small_models,\n",
    "                                          x_key=plot_key, log_axis=log_axis,\n",
    "                                          title=f\"small models, NGRAM={N_GRAMS}, ALL INSTANCES\")\n",
    "avg_scores_all = plot_average_scores_plotly(dataset_scores_all, model_scores_all,\n",
    "                                    color_mapping, None,\n",
    "                                    x_key=plot_key, log_axis=log_axis,\n",
    "                                    title=f\"all models, NGRAM={N_GRAMS}, ALL INSTANCES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa.plot_scores(model_scores, dataset_scores, \n",
    "#             color_mapping, model_color_mapping, \n",
    "#             name=\"XY_pairs\", log_axis=LOG_AXIS,\n",
    "#             x_key=\"coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in task_dfs_all['world_religions']['example']:\n",
    "    print(example['question'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = list(task_dfs_all['world_religions']['example'])\n",
    "examples[5]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the contents of results.json for each model/dataset pair\n",
    "results_dict = collections.defaultdict(dict)\n",
    "instance_results_dict = collections.defaultdict(dict)\n",
    "\n",
    "# Iterate over each model and dataset, loading the results.json file\n",
    "\n",
    "for base_results_path, models_ in base_results_paths.items():\n",
    "    for model in models_:\n",
    "        results_path = os.path.join(base_results_path, model, TASK, SHOT, 'results.json')\n",
    "        instance_results_path = os.path.join(base_results_path, model, TASK, SHOT, 'doc_results.json')\n",
    "        results = json.load(open(results_path, 'r'))['results']\n",
    "        doc_results = json.load(open(instance_results_path, 'r'))\n",
    "        for task in results.keys():\n",
    "            # remove hendrycksTest-\n",
    "            task_str = task.split(\"-\")[1]\n",
    "            if task_str in TASKS:\n",
    "                results_dict[task_str][model] = results[task]['acc']\n",
    "                if os.path.exists(instance_results_path):\n",
    "                    instance_results_dict[model][task_str] = doc_results[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_results_dict['pythia-12b']['world_religions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_instance_results(instance_results, task_dfs, label=''):\n",
    "    model_instance_results = collections.defaultdict(list)\n",
    "    for model in instance_results.keys():\n",
    "        for i, task in enumerate(instance_results[model].keys()):\n",
    "            model_task_results = instance_results[model][task].copy()\n",
    "            # Add task label to each id, with an optional common label\n",
    "            for row in model_task_results:\n",
    "                row['id'] = f\"{str(row['id'])}_{i}\"\n",
    "            model_instance_results[model].extend(model_task_results)\n",
    "    return model_instance_results\n",
    "\n",
    "def merge_and_process_dfs(task_dfs):\n",
    "    example_dfs = pd.concat(task_dfs.values())\n",
    "    # Extract 'query' from the 'example' column\n",
    "    example_dfs['query'] = example_dfs['example'].apply(lambda x: x['question'])\n",
    "    # Perform the aggregation to get the sum of 'value' and other statistics if needed\n",
    "    aggregated_data = example_dfs.groupby(['query', 'task'])['value'].agg(['sum', 'count']).reset_index()\n",
    "    # Merge the aggregated data back to the original DataFrame\n",
    "    example_dfs = example_dfs.merge(aggregated_data, on=['query', 'task'], how='left')\n",
    "    # Sort the DataFrame based on the 'sum' of 'value'\n",
    "    example_dfs = example_dfs.sort_values(by='sum', ascending=False)\n",
    "    # rename \"pair\" column to query\n",
    "    # example_dfs = example_dfs.rename(columns={'pair': 'query'})\n",
    "    \n",
    "    # example_dfs = example_dfs.groupby(['query', 'task'])['value'].sum().sort_values(ascending=False).reset_index()\n",
    "    return example_dfs\n",
    "\n",
    "# Process all tasks\n",
    "model_instance_results_all = process_instance_results(instance_results_dict.copy(), task_dfs_all.copy(), label='_all')\n",
    "example_dfs_all = merge_and_process_dfs(task_dfs_all)\n",
    "\n",
    "# Process common tasks\n",
    "model_instance_results_common = process_instance_results(instance_results_dict.copy(), task_dfs_common.copy(), label='_common')\n",
    "example_dfs_common = merge_and_process_dfs(task_dfs_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rapidfuzz import process, fuzz\n",
    "\n",
    "# # Create a new column for the id\n",
    "# example_dfs_all['id'] = None\n",
    "# instances = pd.DataFrame(model_instance_results['pythia-12b'])\n",
    "\n",
    "# for instance in tqdm(instances):\n",
    "#     # Skip if this instance has already been matched\n",
    "#     # if example_dfs_all.loc[example_dfs_all['query'] == instance['query'], 'id'].notna().any():\n",
    "#         # continue\n",
    "\n",
    "#     # Get the best match in example_dfs_all['query']\n",
    "#     best_match, score, _ = process.extractOne(instance['query'], example_dfs_all['query'], scorer=fuzz.token_sort_ratio)\n",
    "    \n",
    "#     # If the score is above a certain threshold, assign the id\n",
    "#     example_dfs_all.loc[example_dfs_all['query'] == best_match, 'id'] = instance['id']\n",
    "\n",
    "# # Save the DataFrame to a CSV file in BASE_PATH\n",
    "# example_dfs_all.to_csv(os.path.join(BASE_PATH, 'example_dfs_all_.csv'), index=False)\n",
    "\n",
    "# # instances = model_instance_results['pythia-12b']\n",
    "# # for row, example in enumerate(tqdm(example_dfs_all['query'])):\n",
    "# #     for instance in instances:\n",
    "# #         if example == instance['query']:\n",
    "# #             print(f\"Found match for {example}\")\n",
    "# #             example_dfs_all.loc[row, 'id'] = instance['id']\n",
    "# #             instances.remove(instance)\n",
    "# #             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def normalize_text(text):\n",
    "#     # Convert to lowercase, strip whitespace, remove punctuation, etc.\n",
    "#     return text.lower().strip()\n",
    "\n",
    "# def match_and_update_ids(example_dfs, model_instance_results, filename, key='query'):\n",
    "#     # Convert model_instance_results to a DataFrame\n",
    "#     instances = pd.DataFrame(model_instance_results)\n",
    "\n",
    "#     # Normalize the 'query' column in both DataFrames\n",
    "#     example_dfs[key] = example_dfs[key].apply(normalize_text)\n",
    "#     instances[key] = instances[key].apply(normalize_text)\n",
    "\n",
    "#     print(f\"example_dfs: {example_dfs.head(1)}\")\n",
    "#     print(f\"instances: {instances.head(1)}\")\n",
    "#     # Merge the example_dfs with instances on 'query' column\n",
    "#     merged_df = example_dfs.merge(instances[['id', key]], on=key, how='left')\n",
    "\n",
    "#     # Save the merged DataFrame to a CSV file if it doesn't exist\n",
    "#     if not os.path.exists(filename):\n",
    "#         merged_df.to_csv(filename, index=False)\n",
    "    \n",
    "#     return merged_df\n",
    "\n",
    "# # Assuming BASE_PATH is defined\n",
    "# # Process all tasks\n",
    "# example_dfs_all_filename = os.path.join(BASE_PATH, 'example_dfs_all_exact.csv')\n",
    "# example_dfs_all = match_and_update_ids(example_dfs_all, model_instance_results_all['pythia-12b'], example_dfs_all_filename)\n",
    "\n",
    "# # Process common tasks\n",
    "# example_dfs_common_filename = os.path.join(BASE_PATH, 'example_dfs_common_exact.csv')\n",
    "# example_dfs_common = match_and_update_ids(example_dfs_common, model_instance_results_common['pythia-12b'], example_dfs_common_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "def find_matching_id(row, instances_df):\n",
    "    # Use regex=False to avoid interpreting the pattern as a regular expression\n",
    "    match = instances_df[instances_df['query'].str.contains(row['query'], na=False, regex=False)]\n",
    "    if not match.empty:\n",
    "        return match.iloc[0]['id']\n",
    "    return None\n",
    "\n",
    "def add_ids_and_save(example_dfs, instances, filename):\n",
    "    # Add a new column 'id' to example_dfs by applying the find_matching_id function with a progress bar\n",
    "    example_dfs['id'] = example_dfs.progress_apply(find_matching_id, axis=1, instances_df=instances)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    example_dfs.to_csv(filename, index=False)\n",
    "\n",
    "    return example_dfs\n",
    "\n",
    "# Assuming model_instance_results is a list of dictionaries\n",
    "instances_all_df = pd.DataFrame(model_instance_results_all['pythia-12b'])\n",
    "instances_common_df = pd.DataFrame(model_instance_results_common['pythia-12b'])\n",
    "\n",
    "# Assuming BASE_PATH is defined\n",
    "example_dfs_all_filename = os.path.join(BASE_PATH, 'example_dfs_all_exact.csv')\n",
    "example_dfs_common_filename = os.path.join(BASE_PATH, 'example_dfs_common_exact.csv')\n",
    "\n",
    "# Process all tasks\n",
    "# example_dfs_all = add_ids_and_save(example_dfs_all, instances_all_df, example_dfs_all_filename)\n",
    "\n",
    "# Process common tasks\n",
    "example_dfs_common = add_ids_and_save(example_dfs_common, instances_common_df, example_dfs_common_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it\n",
    "# example_dfs_all_0 = pd.read_csv(os.path.join(BASE_PATH, \"_0\", \"example_dfs_all_exact_0.csv\"))\n",
    "# example_dfs_common_0 = pd.read_csv(os.path.join(BASE_PATH, \"_0\", \"example_dfs_common_exact_0.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load it\n",
    "# examples_dfs_all = pd.read_csv(os.path.join(BASE_PATH, 'example_dfs_all_exact.csv'))\n",
    "# examples_dfs_common = pd.read_csv(os.path.join(BASE_PATH, 'example_dfs_common_exact.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # subtract max(x) for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def nll_to_prob(nll):\n",
    "    return np.exp(nll)\n",
    "\n",
    "def process_and_save_results(example_dfs, model_instance_results, base_path, suffix):\n",
    "    examples_dfs_models = {}\n",
    "    for model in model_instance_results.keys():\n",
    "        instances = pd.DataFrame(model_instance_results[model])\n",
    "        examples_dfs_model = example_dfs.merge(instances, on='id', how='inner')\n",
    "        examples_dfs_model['probs_softmax'] = examples_dfs_model['lls'].apply(softmax)\n",
    "        examples_dfs_model['probs_nll'] = examples_dfs_model['lls'].apply(nll_to_prob)\n",
    "        examples_dfs_model['probs_softmax_gold'] = examples_dfs_model.apply(lambda row: row['probs_softmax'][row['gold']], axis=1)\n",
    "        examples_dfs_model['probs_nll_gold'] = examples_dfs_model.apply(lambda row: row['probs_nll'][row['gold']], axis=1)\n",
    "        examples_dfs_models[model] = examples_dfs_model\n",
    "\n",
    "    # Save model_instance_results to pickle\n",
    "    with open(os.path.join(base_path, f'model_instance_results_{suffix}.pkl'), 'wb') as f:\n",
    "        pickle.dump(model_instance_results, f)\n",
    "\n",
    "    # Save examples_dfs_models to pickle\n",
    "    with open(os.path.join(base_path, f'examples_dfs_{suffix}_models.pkl'), 'wb') as f:\n",
    "        pickle.dump(examples_dfs_models, f)\n",
    "    \n",
    "    # # save example_dfs to pkl\n",
    "    # with open(os.path.join(base_path, f'example_dfs_{suffix}.pkl'), 'wb') as f:\n",
    "    #     pickle.dump(example_dfs, f)\n",
    "\n",
    "    return examples_dfs_models\n",
    "    \n",
    "    \n",
    "\n",
    "# Assuming BASE_PATH is defined\n",
    "# Process all tasks\n",
    "# example_dfs_all_models = process_and_save_results(example_dfs_all, model_instance_results_all, BASE_PATH, 'all')\n",
    "\n",
    "# Process common tasks\n",
    "example_dfs_common_models = process_and_save_results(example_dfs_common, model_instance_results_common, BASE_PATH, 'common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many examples are not None or Nan\n",
    "examples_matched = example_dfs_all['id'].notna().sum()\n",
    "examples_umatched = example_dfs_all['id'].isna().sum()\n",
    "\n",
    "print(f\"Examples matched: {examples_matched}\")\n",
    "print(f\"Examples unmatched: {examples_umatched}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
