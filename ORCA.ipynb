{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/models/pythia/experiment_1/huggingface/flan_v1/c4_mixed_NLI/EleutherAI/pythia-160M-deduped_ckpt_False/checkpoint-70000\"\n",
    "dataset_path = '/share/edc/home/antonis/datasets/huggingface/flan_v1_task_ds_n_5000'\n",
    "device = \"cpu\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from datasets import load_from_disk\n",
    "\n",
    "set_seed(420)\n",
    "text_column_name = \"text\"\n",
    "def tokenize_function(examples):\n",
    "        with CaptureLogger(tok_logger) as cl:\n",
    "            output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "                \" before being passed to the model.\"\n",
    "            )\n",
    "        return output\n",
    "\n",
    "def load_model(model_name_or_path, device=device):\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, config=config)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, config=config)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return config, tokenizer, model\n",
    "\n",
    "_, tokenizer, model = load_model(model_name_or_path)\n",
    "\n",
    "raw_dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for x in raw_dataset:\n",
    "    if isinstance(raw_dataset, datasets.dataset_dict.DatasetDict):\n",
    "        pth = os.path.join(dataset_2_path, x)\n",
    "        assert os.path.exists(pth), f\"Dataset {x} not found at {pth}\"\n",
    "        tokenized_datasets[x] = load_from_disk(pth)\n",
    "    else:\n",
    "        tokenized_datasets = load_from_disk(dataset_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenized_datasets['NLI'][0]['inputs']\n",
    "# inputs = tokenizer(inputs, return_tensors='pt')\n",
    "\n",
    "# # Shift the input_ids one token to the right to create the labels\n",
    "# labels = inputs['input_ids'].clone()\n",
    "# labels[:-1] = inputs['input_ids'][1:]\n",
    "\n",
    "# # Calculate the loss\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "import torch.nn as nn\n",
    "\n",
    "# Convert your model to a functional model\n",
    "fmodel, params, buffers = make_functional_with_buffers(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a function to compute the loss of the model given a single input\n",
    "def compute_loss_stateless_model(params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    outputs = fmodel(params, buffers, batch)\n",
    "    loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
    "    return loss\n",
    "\n",
    "# Create a new function that computes the gradient with respect to the params\n",
    "ft_compute_grad = grad(compute_loss_stateless_model)\n",
    "\n",
    "# Use vmap to get the function to compute the gradient over an entire batch of samples and targets\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "\n",
    "\n",
    "# Extract the input_ids tensor from the BatchEncoding object\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Compute per-sample-gradients\n",
    "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
