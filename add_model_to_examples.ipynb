{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from src.wimbd_ import BasePaths as PATHS\n",
    "from src.wimbd_ import DataConfigs as CONFIG\n",
    "from src.wimbd_ import post_filter\n",
    "from src.utils import softmax\n",
    "from wimbd_process import find_best_match\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "# Generate a timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    pattern = r'(.*?D\\..*?)(?:\\nAnswer:|$)'\n",
    "    # Find all matches and keep only up to \"D.\"\n",
    "    cleaned_text = re.sub(pattern, r'\\1', text, flags=re.DOTALL)\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = \"common\"\n",
    "BASE_PATH = \"/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/results/n-grams/mmlu/pile/exp4_filter/test-set/exp_full_None/5\"\n",
    "dfs_all_models_pth = os.path.join(BASE_PATH, f\"examples_dfs_{METHOD}_models.pkl\")\n",
    "df_all_models = pickle.load(open(dfs_all_models_pth, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(df_all_models.keys())\n",
    "df_all_model = df_all_models[models[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Consider the following code segment, which uses the variables r, s, and t.\\n  r ← 1\\n   s ← 2\\n   t ← 3\\n   r ← s\\n   s ← t\\n   DISPLAY (r)\\n   DISPLAY (s)\\n\\n What is displayed as a result of running the code segment?',\n",
       " 'choices': ['1 1', '1 2', '2 3', '3 2'],\n",
       " 'answer': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "df_all_model.iloc[0]['example']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tasks:   0%|          | 0/57 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_new_model_cols: Index(['query', 'lls', 'gold', 'probs', 'probs_gold'], dtype='object')\n",
      "df_model_cols: Index(['Q', 'A', 'value', 'example', 'task', 'pair', 'coverage', 'query',\n",
      "       'sum', 'count', 'id', 'gold', 'query_drop', 'choices', 'lls', 'probs',\n",
      "       'probs_gold'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:45<37:22, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match: ('Statement 1 | Some abelian group of order 45 has a subgroup of order 10. Statement 2 | A subgroup H of a group G is a normal subgroup if and only if thenumber of left cosets of H is equal to the number of right cosets of H.', 100.0, 811235)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [01:18<43:45, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match: ('Find the characteristic of the ring Z_3 x 3Z.', 100.0, 811367)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:51<46:29, 29.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match: ('Find all cosets of the subgroup 4Z of 2Z.', 100.0, 801059)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [02:26<49:45, 31.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match: ('Find all zeros in the indicated finite field of the given polynomial with coefficients in that field. x^3 + 2x + 2 in Z_7', 100.0, 810296)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [03:11<56:13, 35.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match: ('Statement 1 | Every field is also a ring. Statement 2 | Every ring has a multiplicative identity.', 100.0, 799809)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [03:57<1:00:27, 39.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match: ('Statement 1 | Every solvable group is of prime-power order. Statement 2 | Every group of prime-power order is solvable.', 100.0, 800275)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [04:12<48:15, 31.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_match: ('Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the index of <p> in S_5.', 100.0, 48256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [04:14<48:45, 31.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 81\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_updated_model\n\u001b[1;32m     80\u001b[0m new_model_results_pth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/experiment_5/inference/allenai/open-instruct-pythia-6.9b-tulu/MMLU/hendrycks*/0-shot/doc_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 81\u001b[0m df_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_new_df_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model_results_pth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_all_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 53\u001b[0m, in \u001b[0;36mcreate_new_df_model\u001b[0;34m(json_file, df_model)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Apply the match_row function to each row in df_model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m df_new_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_new_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Select only the necessary columns from df_new_model for the update\u001b[39;00m\n\u001b[1;32m     56\u001b[0m df_new_model_relevant \u001b[38;5;241m=\u001b[39m df_new_model[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs_gold\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/tqdm/std.py:805\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9422\u001b[0m )\n\u001b[0;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/incidental/lib/python3.11/site-packages/tqdm/std.py:800\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    799\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 53\u001b[0m, in \u001b[0;36mcreate_new_df_model.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Apply the match_row function to each row in df_model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m df_new_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_new_model\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmatch_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Select only the necessary columns from df_new_model for the update\u001b[39;00m\n\u001b[1;32m     56\u001b[0m df_new_model_relevant \u001b[38;5;241m=\u001b[39m df_new_model[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs_gold\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[51], line 44\u001b[0m, in \u001b[0;36mcreate_new_df_model.<locals>.match_row\u001b[0;34m(row, instances_df, instances_key)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch_row\u001b[39m(row, instances_df, instances_key):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Use the find_best_match function to find a match for the 'query'\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     match_df \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstances_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstances_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# If a match is found, return the ID from the match\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m match_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/share/edc/home/antonis/LLM-Incidental-Supervision/incidental-supervision/wimbd_process.py:369\u001b[0m, in \u001b[0;36mfind_best_match\u001b[0;34m(example, instances_df, instances_key, threshold)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# If no match is found, perform fuzzy matching\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m--> 369\u001b[0m     best_match \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractOne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstances_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuzz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_set_ratio\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_match\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# Check if the best match score is above the specified threshold\u001b[39;00m\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:835\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.extractOne\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:477\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.extractOne_dict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/rapidfuzz/process_cpp_impl.pyx:288\u001b[0m, in \u001b[0;36mrapidfuzz.process_cpp_impl.extractOne_dict_f64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_new_df_model(json_file, df_model):\n",
    "    \"\"\"\n",
    "    Update the DataFrame df_model with new model results from a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        df_model (DataFrame): The DataFrame to be updated.\n",
    "        json_file (str): The path to the JSON file containing the new model results.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The updated DataFrame with the new model results.\n",
    "    \"\"\"\n",
    "    # Load the JSON file containing the new model results\n",
    "    with open(json_file) as f:\n",
    "        new_model_results = json.load(f)\n",
    "\n",
    "\n",
    "    # Flatten the new model results and prepare for DataFrame conversion\n",
    "    flat_results = []\n",
    "    for task, results in tqdm(new_model_results.items(), desc=\"Processing tasks\"):\n",
    "        for result in results:\n",
    "            query = clean_text(result[\"query\"])\n",
    "            flat_results.append({\n",
    "                \"query\": query,\n",
    "                \"lls\": result[\"result\"],\n",
    "                \"gold\": result[\"gold\"],\n",
    "                \"probs\": np.exp(result[\"result\"]),  # Assuming \"result\" contains the nlls\n",
    "                \"probs_gold\": np.exp(result[\"result\"][result[\"gold\"]])\n",
    "            })\n",
    "        break\n",
    "\n",
    "    # Create a DataFrame from the flattened new model results\n",
    "    df_new_model = pd.DataFrame(flat_results)\n",
    "\n",
    "    print(f\"df_new_model_cols: {df_new_model.columns}\")\n",
    "    print(f\"df_model_cols: {df_model.columns}\")\n",
    "\n",
    "    # Convert 'query' columns to strings and strip whitespace\n",
    "    df_model['query'] = df_model['query'].astype(str).str.strip()\n",
    "    df_new_model['query'] = df_new_model['query'].astype(str).str.strip()\n",
    "\n",
    "    # Define a function to apply the matching process row-wise\n",
    "    def match_row(row, instances_df, instances_key):\n",
    "        # Use the find_best_match function to find a match for the 'query'\n",
    "        match_df = find_best_match(row['query'], instances_df, instances_key)\n",
    "        if not match_df.empty:\n",
    "            # If a match is found, return the ID from the match\n",
    "            return match_df.iloc[0]['id']\n",
    "        else:\n",
    "            # If no match is found, return None or some indicator\n",
    "            return None\n",
    "\n",
    "    # Apply the match_row function to each row in df_model\n",
    "    df_new_model['id'] = df_new_model.progress_apply(lambda x: match_row(x, df_model, 'query'), axis=1)\n",
    "\n",
    "    # Select only the necessary columns from df_new_model for the update\n",
    "    df_new_model_relevant = df_new_model[['id', 'lls', 'probs', 'probs_gold']].copy()\n",
    "\n",
    "    # drop this cols from df_model\n",
    "    df_model.drop(columns=['lls', 'probs', 'probs_gold'], inplace=True)\n",
    "\n",
    "    # Merge df_model with the relevant columns from df_new_model based on the 'id'\n",
    "    df_updated_model = pd.merge(df_model, df_new_model_relevant, left_on='id', right_on='id', how='left')\n",
    "\n",
    "    # Drop the 'id' column and any columns from df_new_model that are not needed\n",
    "    # df_updated_model.drop(columns=['id', 'id'], inplace=True)\n",
    "    print(f\"columns: {df_updated_model.columns}\")\n",
    "    # Check for rows that only exist in one of the DataFrames\n",
    "    unmatched_rows = df_updated_model[df_updated_model['lls'].isna()]\n",
    "    num_unmatched_rows = len(unmatched_rows)\n",
    "\n",
    "    # Report unmatched rows\n",
    "    if num_unmatched_rows > 0:\n",
    "        print(f\"Number of unmatched rows: {num_unmatched_rows}\")\n",
    "    else:\n",
    "        print(\"All rows matched successfully.\")\n",
    "\n",
    "    return df_updated_model\n",
    "\n",
    "\n",
    "new_model_results_pth = \"./models/experiment_5/inference/allenai/open-instruct-pythia-6.9b-tulu/MMLU/hendrycks*/0-shot/doc_results.json\"\n",
    "df_model = create_new_df_model(new_model_results_pth, df_all_model)\n",
    "\n",
    "# save the results\n",
    "new_model_name = \"open-instruct-pythia-6.9b-tulu\"\n",
    "df_all_models[new_model_name] = df_model\n",
    "\n",
    "dfs_all_models_pth_new = os.path.join(BASE_PATH, f\"examples_dfs_{METHOD}_models_sft_6_9pythia.pkl\")\n",
    "with open(dfs_all_models_pth, \"wb\") as f:\n",
    "    pickle.dump(df_all_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "incidental",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
